{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random_normal(shape=[16,28,28,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.random_normal(shape=[16,28,28,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(network_variables, name):\n",
    "    \"\"\"\n",
    "    This method counts the tota\n",
    "    l number of unique parameters for a list of variable objects\n",
    "    :param network_variables: A list of tf network variable objects\n",
    "    :param name: Name of the network\n",
    "    \"\"\"\n",
    "    total_parameters = 0\n",
    "    for variable in network_variables:\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            variable_parametes *= dim.value\n",
    "\n",
    "        total_parameters += variable_parametes\n",
    "    print(name, total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, batch_size, layer_sizes, inner_layers, use_wide_connections=False, name=\"d\"):\n",
    "        \"\"\"\n",
    "        Initialize a discriminator network.\n",
    "        :param batch_size: Batch size for discriminator.\n",
    "        :param layer_sizes: A list with the feature maps for each MultiLayer.\n",
    "        :param inner_layers: An integer indicating the number of inner layers.\n",
    "        \"\"\"\n",
    "        self.reuse = False\n",
    "        self.batch_size = batch_size\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.inner_layers = inner_layers\n",
    "        self.conv_layer_num = 0\n",
    "        self.use_wide_connections = use_wide_connections\n",
    "        self.build = True\n",
    "        self.name = name\n",
    "        self.current_layers = []\n",
    "        \n",
    "    def upscale(self, x, scale):\n",
    "        \"\"\"\n",
    "            Upscales an image using nearest neighbour\n",
    "            :param x: Input image\n",
    "            :param h_size: Image height size\n",
    "            :param w_size: Image width size\n",
    "            :return: Upscaled image\n",
    "        \"\"\"\n",
    "        [b, h, w, c] = [int(dim) for dim in x.get_shape()]\n",
    "\n",
    "        return tf.image.resize_nearest_neighbor(x, (h * scale, w * scale))\n",
    "        \n",
    "    def encoder_block(self, x1, num): #last output, previous_output \n",
    "    #with tf.variable_scope(np.str(num)+'encoder_block'):\n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                            num_outputs = 64, padding = 'SAME',\n",
    "                            kernel_size = [3,3], stride = (1,1),\n",
    "                            activation_fn = tf.nn.leaky_relu,\n",
    "                            normalizer_fn=slim.batch_norm, normalizer_params= self.batch_norm_params):\n",
    "\n",
    "            conv1_1 = slim.conv2d(x1)\n",
    "            self.current_layers.append(conv1_1)\n",
    "            output1_1 = tf.concat([conv1_1, x1], axis=3)\n",
    "            print(output1_1)\n",
    "\n",
    "            conv1_2 = slim.conv2d(output1_1)\n",
    "            self.current_layers.append(conv1_2)\n",
    "            output1_2 = tf.concat([conv1_2, output1_1], axis=3)\n",
    "            print(output1_2)\n",
    "\n",
    "            conv1_3 = slim.conv2d(output1_2)\n",
    "            self.current_layers.append(conv1_3)\n",
    "            output1_3 = tf.concat([conv1_3, output1_2], axis=3)\n",
    "            print(output1_3)\n",
    "\n",
    "            conv1_4 = slim.conv2d(output1_3)\n",
    "            self.current_layers.append(conv1_4)\n",
    "            output1_4 = tf.concat([conv1_4, output1_3], axis=3)\n",
    "            print(output1_4)\n",
    "\n",
    "            conv1_5 = slim.conv2d(output1_4, stride=(2,2))\n",
    "            self.current_layers.append(conv1_5)\n",
    "            output = slim.dropout(conv1_5, keep_prob=0.5)\n",
    "            self.current_layers.append(output)\n",
    "            print(output)\n",
    "\n",
    "            if num == 3:\n",
    "                pass\n",
    "            else :\n",
    "                input_projection = slim.conv2d(conv1_4, num_outputs=conv1_4.get_shape()[3], stride=(2,2),\n",
    "                                               activation_fn= None, normalizer_fn= None)\n",
    "                output = tf.concat([output, input_projection], axis=3)\n",
    "            print(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def __call__(self, conditional_input, generated_input, training=False, dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        :param conditional_input: A batch of conditional inputs (x_i) of size [batch_size, height, width, channel]\n",
    "        :param generated_input: A batch of generated inputs (x_g) of size [batch_size, height, width, channel]\n",
    "        :param training: Placeholder for training or a boolean indicating training or validation\n",
    "        :param dropout_rate: A float placeholder for dropout rate or a float indicating the dropout rate\n",
    "        :param name: Network name\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.batch_norm_params = {'decay' : 0.99, 'scale' : True, 'center' : True,\n",
    "                                 'is_training' : training, 'renorm' : True}\n",
    "        conditional_input = tf.convert_to_tensor(conditional_input)\n",
    "        generated_input = tf.convert_to_tensor(generated_input)\n",
    "        \n",
    "        \n",
    "        #with tf.variable_scope(self.name, reuse=self.reuse):\n",
    "        concat_images = tf.concat([conditional_input, generated_input], axis=3)\n",
    "        outputs = concat_images\n",
    "        self.current_layers.append(outputs)\n",
    "               \n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                        num_outputs = 64, padding = 'SAME',\n",
    "                        kernel_size = [3,3], stride = (1,1),\n",
    "                        activation_fn = tf.nn.leaky_relu,\n",
    "                        normalizer_fn=slim.batch_norm, normalizer_params= self.batch_norm_params):\n",
    "            \n",
    "            #with tf.variable_scope('first_conv'): \n",
    "            conv = slim.conv2d(outputs, stride=(2,2))\n",
    "            self.current_layers.append(conv)\n",
    "\n",
    "            input_projection = slim.conv2d(outputs, num_outputs=outputs.get_shape()[3], kernel_size = [3,3], stride=(2,2),\n",
    "                                   activation_fn= None, normalizer_fn= None)\n",
    "            conv1 = tf.concat([conv, input_projection], axis=3)\n",
    "\n",
    "            en1 = self.encoder_block(conv1, 1)\n",
    "            en2 = self.encoder_block(en1, 2)\n",
    "            en3 = self.encoder_block(en2, 3)\n",
    "\n",
    "        #with tf.variable_scope('decoder_block'):\n",
    "            feature_level_flatten = tf.reduce_mean(en3, axis=[1, 2])\n",
    "            print('else_feature_level_flatten : ', feature_level_flatten)\n",
    "            location_level_flatten = tf.layers.flatten(en3)\n",
    "            print('else_location_level_flatten : ', location_level_flatten)\n",
    "\n",
    "            feature_level_dense = tf.layers.dense(feature_level_flatten, units=1024, activation=tf.nn.leaky_relu)\n",
    "            combo_level_flatten = tf.concat([feature_level_dense, location_level_flatten], axis=1)\n",
    "\n",
    "        #with tf.variable_scope('discriminator_out_block'):\n",
    "            outputs = tf.layers.dense(combo_level_flatten, 1, name='outputs')\n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        \n",
    "        if self.build:\n",
    "            print(\"discr layers\", self.conv_layer_num)\n",
    "            count_parameters(self.variables, name=\"discriminator_parameter_num\")\n",
    "        self.build = False\n",
    "        \n",
    "        return outputs, self.current_layers\n",
    "                \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Discriminator(16, [64,64,64,64], [5,5,5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat_1:0\", shape=(16, 14, 14, 134), dtype=float32)\n",
      "Tensor(\"concat_2:0\", shape=(16, 14, 14, 198), dtype=float32)\n",
      "Tensor(\"concat_3:0\", shape=(16, 14, 14, 262), dtype=float32)\n",
      "Tensor(\"concat_4:0\", shape=(16, 14, 14, 326), dtype=float32)\n",
      "Tensor(\"Dropout/dropout_1/mul:0\", shape=(16, 7, 7, 64), dtype=float32)\n",
      "Tensor(\"concat_5:0\", shape=(16, 7, 7, 128), dtype=float32)\n",
      "Tensor(\"concat_6:0\", shape=(16, 7, 7, 192), dtype=float32)\n",
      "Tensor(\"concat_7:0\", shape=(16, 7, 7, 256), dtype=float32)\n",
      "Tensor(\"concat_8:0\", shape=(16, 7, 7, 320), dtype=float32)\n",
      "Tensor(\"concat_9:0\", shape=(16, 7, 7, 384), dtype=float32)\n",
      "Tensor(\"Dropout_1/dropout/mul:0\", shape=(16, 4, 4, 64), dtype=float32)\n",
      "Tensor(\"concat_10:0\", shape=(16, 4, 4, 128), dtype=float32)\n",
      "Tensor(\"concat_11:0\", shape=(16, 4, 4, 192), dtype=float32)\n",
      "Tensor(\"concat_12:0\", shape=(16, 4, 4, 256), dtype=float32)\n",
      "Tensor(\"concat_13:0\", shape=(16, 4, 4, 320), dtype=float32)\n",
      "Tensor(\"concat_14:0\", shape=(16, 4, 4, 384), dtype=float32)\n",
      "Tensor(\"Dropout_2/dropout/mul:0\", shape=(16, 2, 2, 64), dtype=float32)\n",
      "Tensor(\"Dropout_2/dropout/mul:0\", shape=(16, 2, 2, 64), dtype=float32)\n",
      "else_feature_level_flatten :  Tensor(\"Mean:0\", shape=(16, 64), dtype=float32)\n",
      "else_location_level_flatten :  Tensor(\"flatten/Reshape:0\", shape=(16, 256), dtype=float32)\n",
      "discr layers 0\n",
      "discriminator_parameter_num 2192331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'outputs/BiasAdd:0' shape=(16, 1) dtype=float32>,\n",
       " [<tf.Tensor 'd/concat:0' shape=(16, 28, 28, 6) dtype=float32>,\n",
       "  <tf.Tensor 'Conv/LeakyRelu:0' shape=(16, 14, 14, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_2/LeakyRelu:0' shape=(16, 14, 14, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_3/LeakyRelu:0' shape=(16, 14, 14, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_4/LeakyRelu:0' shape=(16, 14, 14, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_5/LeakyRelu:0' shape=(16, 14, 14, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_6/LeakyRelu:0' shape=(16, 7, 7, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Dropout/dropout_1/mul:0' shape=(16, 7, 7, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_8/LeakyRelu:0' shape=(16, 7, 7, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_9/LeakyRelu:0' shape=(16, 7, 7, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_10/LeakyRelu:0' shape=(16, 7, 7, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_11/LeakyRelu:0' shape=(16, 7, 7, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_12/LeakyRelu:0' shape=(16, 4, 4, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Dropout_1/dropout/mul:0' shape=(16, 4, 4, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_14/LeakyRelu:0' shape=(16, 4, 4, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_15/LeakyRelu:0' shape=(16, 4, 4, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_16/LeakyRelu:0' shape=(16, 4, 4, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_17/LeakyRelu:0' shape=(16, 4, 4, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Conv_18/LeakyRelu:0' shape=(16, 2, 2, 64) dtype=float32>,\n",
       "  <tf.Tensor 'Dropout_2/dropout/mul:0' shape=(16, 2, 2, 64) dtype=float32>])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(x, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat_17:0\", shape=(16, 14, 14, 134), dtype=float32)\n",
      "Tensor(\"concat_18:0\", shape=(16, 14, 14, 198), dtype=float32)\n",
      "Tensor(\"concat_19:0\", shape=(16, 14, 14, 262), dtype=float32)\n",
      "Tensor(\"concat_20:0\", shape=(16, 14, 14, 326), dtype=float32)\n",
      "Tensor(\"Dropout_3/dropout/mul:0\", shape=(16, 7, 7, 64), dtype=float32)\n",
      "Tensor(\"concat_21:0\", shape=(16, 7, 7, 128), dtype=float32)\n",
      "Tensor(\"concat_22:0\", shape=(16, 7, 7, 192), dtype=float32)\n",
      "Tensor(\"concat_23:0\", shape=(16, 7, 7, 256), dtype=float32)\n",
      "Tensor(\"concat_24:0\", shape=(16, 7, 7, 320), dtype=float32)\n",
      "Tensor(\"concat_25:0\", shape=(16, 7, 7, 384), dtype=float32)\n",
      "Tensor(\"Dropout_4/dropout/mul:0\", shape=(16, 4, 4, 64), dtype=float32)\n",
      "Tensor(\"concat_26:0\", shape=(16, 4, 4, 128), dtype=float32)\n",
      "Tensor(\"concat_27:0\", shape=(16, 4, 4, 192), dtype=float32)\n",
      "Tensor(\"concat_28:0\", shape=(16, 4, 4, 256), dtype=float32)\n",
      "Tensor(\"concat_29:0\", shape=(16, 4, 4, 320), dtype=float32)\n",
      "Tensor(\"concat_30:0\", shape=(16, 4, 4, 384), dtype=float32)\n",
      "Tensor(\"Dropout_5/dropout/mul:0\", shape=(16, 2, 2, 64), dtype=float32)\n",
      "Tensor(\"Dropout_5/dropout/mul:0\", shape=(16, 2, 2, 64), dtype=float32)\n",
      "else_feature_level_flatten :  Tensor(\"Mean_1:0\", shape=(16, 64), dtype=float32)\n",
      "else_location_level_flatten :  Tensor(\"flatten_1/Reshape:0\", shape=(16, 256), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable outputs/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-399abf5d795e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-2ce40d1d5d3e>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, conditional_input, generated_input, training, dropout_rate)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;31m#with tf.variable_scope('discriminator_out_block'):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombo_level_flatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'outputs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreuse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\u001b[0m in \u001b[0;36mdense\u001b[1;34m(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0m_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                 _reuse=reuse)\n\u001b[1;32m--> 189\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    803\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m     \"\"\"\n\u001b[1;32m--> 805\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_set_learning_phase_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m         trainable=True)\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       self.bias = self.add_weight(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m             getter=vs.get_variable)\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, getter)\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m         aggregation=aggregation)\n\u001b[0m\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[1;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[0;32m    533\u001b[0m     new_variable = getter(\n\u001b[0;32m    534\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m         **kwargs_for_getter)\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \u001b[1;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1465\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1467\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1215\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    525\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    479\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[1;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    846\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 848\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable outputs/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    a = Discriminator(16, [64,64,64,64], [5,5,5,5])\n",
    "    print(sess.run(a(x,g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
