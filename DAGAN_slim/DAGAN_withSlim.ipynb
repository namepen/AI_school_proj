{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DAGAN UResNetGerenetor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.zeros(shape=[16,28,28,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_input = tf.random_normal([16, 100], mean=0, stddev=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale(x, h_size, w_size):\n",
    "        \"\"\"\n",
    "        Upscales an image using nearest neighbour\n",
    "        :param x: Input image\n",
    "        :param h_size: Image height size\n",
    "        :param w_size: Image width size\n",
    "        :return: Upscaled image\n",
    "        \"\"\"\n",
    "        [b, h, w, c] = [int(dim) for dim in x.get_shape()]\n",
    "\n",
    "        return tf.image.resize_nearest_neighbor(x, (h_size, w_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(network_variables, name):\n",
    "    \"\"\"\n",
    "    This method counts the total number of unique parameters for a list of variable objects\n",
    "    :param network_variables: A list of tf network variable objects\n",
    "    :param name: Name of the network\n",
    "    \"\"\"\n",
    "    total_parameters = 0\n",
    "    for variable in network_variables:\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            variable_parametes *= dim.value\n",
    "\n",
    "        total_parameters += variable_parametes\n",
    "    print(name, total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge = UResnetGenerator(is_training=True, scope='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UResnet(object):\n",
    "    \n",
    "    def __init__(self, layer_sizes, layer_padding, batch_size, num_channels=1,inner_layers=0, name=\"g\"):\n",
    "        \n",
    "        '''\n",
    "        Initialize a UResNet generator.\n",
    "        :param layer_sizes: A list with the filter sizes for each MultiLayer e.g. [64, 64, 128, 128]\n",
    "        :param layer_padding: A list with the padding type for each layer e.g. [\"SAME\", \"SAME\", \"SAME\", \"SAME\"]\n",
    "        :param batch_size: An integer indicating the batch size\n",
    "        :param num_channels: An integer indicating the number of input channels\n",
    "        :param inner_layers: An integer indicating the number of inner layers per MultiLayer\n",
    "        '''\n",
    "        self.reuse = False\n",
    "        self.batch_size = batch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.layer_padding = layer_padding\n",
    "        self.inner_layers = inner_layers\n",
    "        self.conv_layer_num = 0\n",
    "        self.build = True\n",
    "        self.name = name\n",
    "        '''\n",
    "        input = [batch_size, h, w, num_chanels] of tensor, [batch_size, 28, 28, 3]\n",
    "        '''\n",
    "        \n",
    "\n",
    "    \n",
    "    def __call__(self, z_input, input, training=False):\n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                           num_outputs = 64,\n",
    "                            padding = 'SAME',\n",
    "                            kernel_size = [3,3],\n",
    "                            stride = (1,1),\n",
    "                            activation_fn = None):\n",
    "            with slim.arg_scope([slim.batch_norm],\n",
    "                               decay =0.99, scale= True, center=True, is_training=training, renorm=True):\n",
    "\n",
    "                with tf.variable_scope('fisrt_single_conv'): \n",
    "                    conv = slim.conv2d(input, stride=(2,2))\n",
    "                    conv = tf.nn.leaky_relu(conv)\n",
    "                    conv = slim.batch_norm(conv) # [batch_size, 14, 14, 3]\n",
    "\n",
    "                with tf.variable_scope('first_resnet_block'):\n",
    "                    input_projection = slim.conv2d(input, num_outputs=input.get_shape()[3], stride=(2,2))\n",
    "                    output1 = tf.concat([conv, input_projection], axis=3)\n",
    "                    print(output1)\n",
    "\n",
    "                    conv1_1 = slim.conv2d(output1)\n",
    "                    conv1_1 = tf.nn.leaky_relu(conv1_1)\n",
    "                    conv1_1 = slim.batch_norm(conv1_1)\n",
    "\n",
    "                    output2 = tf.concat([conv, conv1_1], axis=3)\n",
    "                    print(output2)\n",
    "\n",
    "                    conv1_2 = slim.conv2d(output2)\n",
    "                    conv1_2 = tf.nn.leaky_relu(conv1_2)\n",
    "                    conv1_2 = slim.batch_norm(conv1_2)\n",
    "\n",
    "                    output3 = tf.concat([conv1_2, output2], axis=3)\n",
    "                    print(output3)\n",
    "\n",
    "                    conv1_3 = slim.conv2d(output3)\n",
    "                    conv1_3 = tf.nn.leaky_relu(conv1_3)\n",
    "                    conv1_3 = slim.batch_norm(conv1_3)\n",
    "\n",
    "                    output4 = tf.concat([conv1_3, output3], axis=3)\n",
    "                    print(output4)\n",
    "\n",
    "                    conv1_4 = slim.conv2d(output4, stride=(2,2))\n",
    "                    conv1_4 = tf.nn.leaky_relu(conv1_4)\n",
    "                    conv1_4 = slim.batch_norm(conv1_4)\n",
    "                    conv1_4 = slim.dropout(conv1_4, keep_prob= 0.5) # [batch_size, 7, 7, 64]\n",
    "\n",
    "                with tf.variable_scope('second_resnet_block'):\n",
    "                    input_projection = slim.conv2d(conv1_3, num_outputs=conv1_3.get_shape()[3], stride=(2,2))\n",
    "                    print(input_projection)\n",
    "                    output5 = tf.concat([conv1_4, input_projection], axis=3)\n",
    "                    print(output5)\n",
    "\n",
    "                    conv2_1 = slim.conv2d(output5)\n",
    "                    conv2_1 = tf.nn.leaky_relu(conv2_1)\n",
    "                    conv2_1 = slim.batch_norm(conv2_1) #x4\n",
    "\n",
    "                    output6 = tf.concat([output5, conv2_1], axis=3)\n",
    "                    print(output6)\n",
    "\n",
    "                    conv2_2 = slim.conv2d(output6)\n",
    "                    conv2_2 = tf.nn.leaky_relu(conv2_2)\n",
    "                    conv2_2 = slim.batch_norm(conv2_2)\n",
    "\n",
    "                    output7 = tf.concat([conv2_2, output6], axis=3)\n",
    "                    print(output7)\n",
    "\n",
    "                    conv2_3 = slim.conv2d(output7)\n",
    "                    conv2_3 = tf.nn.leaky_relu(conv2_3)\n",
    "                    conv2_3 = slim.batch_norm(conv2_3)\n",
    "\n",
    "                    output8 = tf.concat([conv2_3, output7], axis=3)\n",
    "\n",
    "                    conv2_4 = slim.conv2d(output8, stride=(2,2))\n",
    "                    conv2_4 = tf.nn.leaky_relu(conv2_4)\n",
    "                    conv2_4 = slim.batch_norm(conv2_4)\n",
    "                    conv2_4 = slim.dropout(conv2_4, keep_prob= 0.5) # [batch_size, 4, 4, 64]\n",
    "                    print(conv2_4)\n",
    "\n",
    "                with tf.variable_scope('third_resnet_block'):\n",
    "                    input_projection = slim.conv2d(conv2_3, num_outputs=conv2_3.get_shape()[3], stride=(2,2)) #[batch_size, 4, 4, 3]\n",
    "                    print(input_projection)\n",
    "                    output9 = tf.concat([conv2_4, input_projection], axis=3)\n",
    "                    print(output9)\n",
    "\n",
    "                    conv3_1 = slim.conv2d(output9)\n",
    "                    conv3_1 = tf.nn.leaky_relu(conv3_1)\n",
    "                    conv3_1 = slim.batch_norm(conv3_1)\n",
    "\n",
    "                    output3_1 = tf.concat([output9, conv3_1], axis=3)\n",
    "                    print(output3_1)\n",
    "\n",
    "                    conv3_2 = slim.conv2d(output3_1)\n",
    "                    conv3_2 = tf.nn.leaky_relu(conv3_2)\n",
    "                    conv3_2 = slim.batch_norm(conv3_2)\n",
    "\n",
    "                    output3_2 = tf.concat([conv3_2, output3_1], axis=3)\n",
    "                    print(output3_2)\n",
    "\n",
    "                    conv3_3 = slim.conv2d(output3_2)\n",
    "                    conv3_3 = tf.nn.leaky_relu(conv3_3)\n",
    "                    conv3_3 = slim.batch_norm(conv3_3)\n",
    "\n",
    "                    output3_3 = tf.concat([conv3_3, output3_2], axis=3)\n",
    "\n",
    "                    conv3_4 = slim.conv2d(output3_3, stride=(2,2))\n",
    "                    conv3_4 = tf.nn.leaky_relu(conv3_4)\n",
    "                    conv3_4 = slim.batch_norm(conv3_4)\n",
    "                    conv3_4 = slim.dropout(conv3_4, keep_prob= 0.5) # [batch_size, 2, 2, 64]\n",
    "                    conv3_4.get_shape()\n",
    "                    print(conv3_4)\n",
    "                    print('end of encoder')\n",
    "\n",
    "            ##Decoder    \n",
    "                with tf.variable_scope('first_decoder_block'):\n",
    "                    z_dense = tf.layers.dense(z_input, 2*2*8) #num_filters set 8\n",
    "                    z_noise = tf.reshape(z_dense, [input.shape[0], 2, 2, 8])\n",
    "                    d_output1 = tf.concat([conv3_4, z_noise], axis=3) #[batch_size, 2, 2, 72]\n",
    "                    print('first decoder', d_output1)\n",
    "\n",
    "                    d_conv1_1 = slim.conv2d(d_output1)\n",
    "                    d_conv1_1 = tf.nn.leaky_relu(d_conv1_1)\n",
    "                    d_conv1_1 = slim.batch_norm(d_conv1_1)\n",
    "\n",
    "                    d_output1_1 = tf.concat([d_conv1_1, d_output1], axis=3) #[b, 2, 2, 136]\n",
    "\n",
    "                    d_conv1_2 = slim.conv2d(d_output1_1)\n",
    "                    d_conv1_2 = tf.nn.leaky_relu(d_conv1_2)\n",
    "                    d_conv1_2 = slim.batch_norm(d_conv1_2)\n",
    "\n",
    "                    d_output1_2 = tf.concat([d_conv1_2, d_output1_1], axis=3) #[b, 2, 2, 200]\n",
    "\n",
    "                    d_conv1_3 = slim.conv2d(d_output1_2)\n",
    "                    d_conv1_3 = tf.nn.leaky_relu(d_conv1_3)\n",
    "                    d_conv1_3 = slim.batch_norm(d_conv1_3)\n",
    "\n",
    "                    d_output1_3 = tf.concat([d_conv1_3, d_output1_2], axis=3) #[b, 2, 2, 264]\n",
    "\n",
    "                    d_conv1_4 = upscale(d_output1_3, 4, 4)\n",
    "                    d_conv1_4 = slim.conv2d_transpose(d_conv1_4)\n",
    "                    d_conv1_4 = tf.nn.leaky_relu(d_conv1_4)\n",
    "                    d_conv1_4 = slim.batch_norm(d_conv1_4)\n",
    "                    d_conv1_4 = slim.dropout(d_conv1_4, keep_prob= 0.5)\n",
    "\n",
    "                    d_output1_4 = tf.concat([d_conv1_4, conv2_4], axis = 3) #[b, 4, 4, 128]\n",
    "\n",
    "\n",
    "                with tf.variable_scope('second_decoder_block'):\n",
    "                    z_dense = tf.layers.dense(z_input, 4*4*4) #num_filters set 4\n",
    "                    z_noise = tf.reshape(z_dense, [input.shape[0], 4, 4, 4])\n",
    "                    d_output2 = tf.concat([d_output1_4, z_noise], axis=3) #[b, 4, 4, 132]\n",
    "\n",
    "                    input_projection = upscale(d_conv1_3, 4, 4)\n",
    "                    input_projection = slim.conv2d_transpose(input_projection, num_outputs=d_conv1_3.get_shape()[3]) #[batch_size, 4, 4, 64]\n",
    "                    print(input_projection)\n",
    "                    d_output2 = tf.concat([d_output2, input_projection], axis=3) #[b, 4, 4, 196]\n",
    "                    print(d_output2)\n",
    "\n",
    "                    d_conv2_1 = slim.conv2d(d_output2)\n",
    "                    d_conv2_1 = tf.nn.leaky_relu(d_conv2_1)\n",
    "                    d_conv2_1 = slim.batch_norm(d_conv2_1)\n",
    "\n",
    "                    d_output2_1 = tf.concat([d_conv2_1, d_output2], axis=3) #[b, 4, 4, 260]\n",
    "\n",
    "                    d_conv2_2 = slim.conv2d(d_output2_1)\n",
    "                    d_conv2_2 = tf.nn.leaky_relu(d_conv2_2)\n",
    "                    d_conv2_2 = slim.batch_norm(d_conv2_2)\n",
    "\n",
    "                    d_output2_2 = tf.concat([d_conv2_2, d_output2_1], axis=3) #[b, 4, 4, 324]\n",
    "\n",
    "                    d_conv2_3 = slim.conv2d(d_output2_2)\n",
    "                    d_conv2_3 = tf.nn.leaky_relu(d_conv2_3)\n",
    "                    d_conv2_3 = slim.batch_norm(d_conv2_3)\n",
    "\n",
    "                    d_output2_3 = tf.concat([d_conv2_3, d_output2_2], axis=3)\n",
    "\n",
    "                    d_conv2_4 = upscale(d_output2_3, 7, 7)\n",
    "                    d_conv2_4 = slim.conv2d_transpose(d_conv2_4)\n",
    "                    d_conv2_4 = tf.nn.leaky_relu(d_conv2_4)\n",
    "                    d_conv2_4 = slim.batch_norm(d_conv2_4)\n",
    "                    d_conv2_4 = slim.dropout(d_conv2_4, keep_prob= 0.5)\n",
    "\n",
    "                    d_output2_4 = tf.concat([d_conv2_4, conv1_4], axis = 3) #[b, 7, 7, 128]\n",
    "\n",
    "\n",
    "                with tf.variable_scope('third_decoder_block'):\n",
    "                    z_dense = tf.layers.dense(z_input, 7*7*2) #num_filters set 2\n",
    "                    z_noise = tf.reshape(z_dense, [input.shape[0], 7, 7, 2])\n",
    "                    d_output3 = tf.concat([d_output2_4, z_noise], axis=3) #[b, 4, 4, 130]\n",
    "\n",
    "                    input_projection = upscale(d_conv2_3, 7, 7)\n",
    "                    input_projection = slim.conv2d_transpose(input_projection, num_outputs=d_conv2_3.get_shape()[3]) #[batch_size, 4, 4, 64]\n",
    "                    print(input_projection)\n",
    "                    d_output3 = tf.concat([d_output3, input_projection], axis=3) #[b, 4, 4, 196]\n",
    "                    print(d_output3)\n",
    "\n",
    "                    d_conv3_1 = slim.conv2d(d_output3)\n",
    "                    d_conv3_1 = tf.nn.leaky_relu(d_conv3_1)\n",
    "                    d_conv3_1 = slim.batch_norm(d_conv3_1)\n",
    "\n",
    "                    d_output3_1 = tf.concat([d_conv3_1, d_output3], axis=3) #[b, 4, 4, 260]\n",
    "\n",
    "                    d_conv3_2 = slim.conv2d(d_output3_1)\n",
    "                    d_conv3_2 = tf.nn.leaky_relu(d_conv3_2)\n",
    "                    d_conv3_2 = slim.batch_norm(d_conv3_2)\n",
    "\n",
    "                    d_output3_2 = tf.concat([d_conv3_2, d_output3_1], axis=3) #[b, 4, 4, 324]\n",
    "\n",
    "                    d_conv3_3 = slim.conv2d(d_output3_2)\n",
    "                    d_conv3_3 = tf.nn.leaky_relu(d_conv3_3)\n",
    "                    d_conv3_3 = slim.batch_norm(d_conv3_3)\n",
    "\n",
    "                    d_output3_3 = tf.concat([d_conv3_3, d_output3_2], axis=3)\n",
    "\n",
    "                    d_conv3_4 = upscale(d_output3_3, 14, 14)\n",
    "                    d_conv3_4 = slim.conv2d_transpose(d_conv3_4)\n",
    "                    d_conv3_4 = tf.nn.leaky_relu(d_conv3_4)\n",
    "                    d_conv3_4 = slim.batch_norm(d_conv3_4)\n",
    "                    d_conv3_4 = slim.dropout(d_conv3_4, keep_prob= 0.5) #[b, 14, 14, 64]\n",
    "\n",
    "                    d_output3_4 = tf.concat([d_conv3_4, conv], axis = 3) #[b, 14 14, 128]\n",
    "\n",
    "                with tf.variable_scope('forth_decoder_block'):\n",
    "\n",
    "                    input_projection = upscale(d_conv3_3, 14, 14)\n",
    "                    input_projection = slim.conv2d_transpose(input_projection, num_outputs=d_conv3_3.get_shape()[3]) #[batch_size, 4, 4, 64]\n",
    "                    print(input_projection)\n",
    "                    d_output4 = tf.concat([d_output3_4, input_projection], axis=3) #[b, 14, 14, 192]\n",
    "                    print(d_output4)\n",
    "\n",
    "                    d_conv4_1 = slim.conv2d(d_output4)\n",
    "                    d_conv4_1 = tf.nn.leaky_relu(d_conv4_1)\n",
    "                    d_conv4_1 = slim.batch_norm(d_conv4_1)\n",
    "\n",
    "                    d_output4_1 = tf.concat([d_conv4_1, d_output4], axis=3) #[b, 14, 14, 256]\n",
    "\n",
    "                    d_conv4_2 = slim.conv2d(d_output4_1)\n",
    "                    d_conv4_2 = tf.nn.leaky_relu(d_conv4_2)\n",
    "                    d_conv4_2 = slim.batch_norm(d_conv4_2)\n",
    "\n",
    "                    d_output4_2 = tf.concat([d_conv4_2, d_output4_1], axis=3) #[b, 14, 14, 320]\n",
    "\n",
    "                    d_conv4_3 = upscale(d_output4_2, 28, 28)\n",
    "                    d_conv4_3 = slim.conv2d_transpose(d_conv4_3)\n",
    "                    d_conv4_3 = tf.nn.leaky_relu(d_conv4_3)\n",
    "                    d_conv4_3 = slim.batch_norm(d_conv4_3)\n",
    "                    d_conv4_3 = slim.dropout(d_conv4_3, keep_prob= 0.5) #[b, 28, 28, 64]\n",
    "\n",
    "                    d_output4_3 = tf.concat([d_conv4_3, input], axis=3) #[b, 28, 28, 67]\n",
    "\n",
    "\n",
    "                with tf.variable_scope('last_block'):\n",
    "\n",
    "                    input_projection = upscale(d_conv4_3, 28, 28)\n",
    "                    input_projection = slim.conv2d_transpose(input_projection, num_outputs=d_conv4_3.get_shape()[3]) #[b, 28, 28, 64]\n",
    "                    print(input_projection)\n",
    "                    d_output5 = tf.concat([d_output4_3, input_projection], axis=3) #[b, 14, 14, 127]\n",
    "                    print(d_output5)\n",
    "\n",
    "                    d_conv5_1 = slim.conv2d(d_output5)\n",
    "                    d_conv5_1 = tf.nn.leaky_relu(d_conv5_1)\n",
    "                    d_conv5_1 = slim.batch_norm(d_conv5_1)\n",
    "\n",
    "                    d_output5_1 = tf.concat([d_conv5_1, d_output5], axis=3) #[b, 14, 14, 195]\n",
    "\n",
    "                    d_conv5_2 = slim.conv2d(d_output5_1)\n",
    "                    d_conv5_2 = tf.nn.leaky_relu(d_conv5_2)\n",
    "                    d_conv5_2 = slim.batch_norm(d_conv5_2)\n",
    "\n",
    "                    d_output5_2 = tf.concat([d_conv5_2, d_output5_1], axis=3) #[b, 14, 14, 259]\n",
    "\n",
    "                    #In p process\n",
    "                    d_conv5_3 = slim.conv2d(d_output5_2)\n",
    "                    d_conv5_3 = tf.nn.leaky_relu(d_conv5_3)\n",
    "                    d_conv5_3 = slim.batch_norm(d_conv5_3)\n",
    "\n",
    "                    d_conv5_4 = slim.conv2d(d_conv5_3, num_outputs = 3)\n",
    "                    d_conv5_4 = tf.nn.leaky_relu(d_conv5_4)\n",
    "                    d_conv5_4 = slim.batch_norm(d_conv5_4)\n",
    "\n",
    "                with tf.variable_scope('g_tanh'):\n",
    "                    gan_decoder = tf.tanh(d_conv5_4, name='outputs')\n",
    "                    \n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n",
    "\n",
    "        if self.build:\n",
    "            count_parameters(self.variables, 'generator_parameter_num')\n",
    "        self.build = False\n",
    "        \n",
    "        return gan_decoder\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = UResnet(0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.UResnet object at 0x0000019544456F28>\n"
     ]
    }
   ],
   "source": [
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"first_resnet_block/concat:0\", shape=(16, 14, 14, 67), dtype=float32)\n",
      "Tensor(\"first_resnet_block/concat_1:0\", shape=(16, 14, 14, 128), dtype=float32)\n",
      "Tensor(\"first_resnet_block/concat_2:0\", shape=(16, 14, 14, 192), dtype=float32)\n",
      "Tensor(\"first_resnet_block/concat_3:0\", shape=(16, 14, 14, 256), dtype=float32)\n",
      "Tensor(\"second_resnet_block/Conv/BiasAdd:0\", shape=(16, 7, 7, 64), dtype=float32)\n",
      "Tensor(\"second_resnet_block/concat:0\", shape=(16, 7, 7, 128), dtype=float32)\n",
      "Tensor(\"second_resnet_block/concat_1:0\", shape=(16, 7, 7, 192), dtype=float32)\n",
      "Tensor(\"second_resnet_block/concat_2:0\", shape=(16, 7, 7, 256), dtype=float32)\n",
      "Tensor(\"second_resnet_block/Dropout/dropout_1/mul:0\", shape=(16, 4, 4, 64), dtype=float32)\n",
      "Tensor(\"third_resnet_block/Conv/BiasAdd:0\", shape=(16, 4, 4, 64), dtype=float32)\n",
      "Tensor(\"third_resnet_block/concat:0\", shape=(16, 4, 4, 128), dtype=float32)\n",
      "Tensor(\"third_resnet_block/concat_1:0\", shape=(16, 4, 4, 192), dtype=float32)\n",
      "Tensor(\"third_resnet_block/concat_2:0\", shape=(16, 4, 4, 256), dtype=float32)\n",
      "Tensor(\"third_resnet_block/Dropout/dropout_1/mul:0\", shape=(16, 2, 2, 64), dtype=float32)\n",
      "end of encoder\n",
      "first decoder Tensor(\"first_decoder_block/concat:0\", shape=(16, 2, 2, 72), dtype=float32)\n",
      "Tensor(\"second_decoder_block/Conv2d_transpose/BiasAdd:0\", shape=(16, 4, 4, 64), dtype=float32)\n",
      "Tensor(\"second_decoder_block/concat_1:0\", shape=(16, 4, 4, 196), dtype=float32)\n",
      "Tensor(\"third_decoder_block/Conv2d_transpose/BiasAdd:0\", shape=(16, 7, 7, 64), dtype=float32)\n",
      "Tensor(\"third_decoder_block/concat_1:0\", shape=(16, 7, 7, 194), dtype=float32)\n",
      "Tensor(\"forth_decoder_block/Conv2d_transpose/BiasAdd:0\", shape=(16, 14, 14, 64), dtype=float32)\n",
      "Tensor(\"forth_decoder_block/concat:0\", shape=(16, 14, 14, 192), dtype=float32)\n",
      "Tensor(\"last_block/Conv2d_transpose/BiasAdd:0\", shape=(16, 28, 28, 64), dtype=float32)\n",
      "Tensor(\"last_block/concat:0\", shape=(16, 28, 28, 131), dtype=float32)\n",
      "generator_parameter_num 4195870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'g_tanh/outputs:0' shape=(16, 28, 28, 64) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u(z_input, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UResnetGenerator(object):\n",
    "    def __init__(self, is_training=True, scope=None) :\n",
    "        self.scope = scope\n",
    "        self.is_training = is_training\n",
    "        self.output_channels = 3\n",
    "        self.batch_norm_params = {'decay' : 0.99, 'scale' : True, 'center' : True,\n",
    "                                 'is_training' : self.is_training, 'renorm' : True}\n",
    "        \n",
    "    \n",
    "    def encoder_block(self, x1, num): #last output, previous_output \n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                            num_outputs = 64, padding = 'SAME',\n",
    "                            kernel_size = [3,3], stride = (1,1),\n",
    "                            activation_fn = tf.nn.leaky_relu,\n",
    "                            normalizer_fn=slim.batch_norm, normalizer_params= self.batch_norm_params):\n",
    "\n",
    "            conv1_1 = slim.conv2d(x1)\n",
    "            output1_1 = tf.concat([conv1_1, x1], axis=3)\n",
    "            \n",
    "            conv1_2 = slim.conv2d(output1_1)\n",
    "            output1_2 = tf.concat([conv1_2, output1_1], axis=3)\n",
    "            \n",
    "            conv1_3 = slim.conv2d(output1_2)\n",
    "            output1_3 = tf.concat([conv1_3, output1_2], axis=3)\n",
    "            \n",
    "            conv1_4 = slim.conv2d(output1_3)\n",
    "            output = slim.dropout(conv1_4, keep_prob=0.5)\n",
    "            if num == 3:\n",
    "                pass\n",
    "            else :\n",
    "                input_projection = slim.conv2d(output1_3, num_outputs=output1_3.get_shape()[3], stride=(2,2),\n",
    "                                               activation_fn= None, normalizer_fn= None)\n",
    "                output = tf.concat([output, input_projection], axis=3)\n",
    "            print(output)\n",
    "            \n",
    "        return output\n",
    "    '''\n",
    "    def decoder_block(self, x1, z_input, num):\n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                            num_outputs = 64, padding = 'SAME',\n",
    "                            kernel_size = [3,3], stride = (1,1),\n",
    "                            activation_fn = tf.nn.leaky_relu,\n",
    "                            normalizer_fn=slim.batch_norm, normalizer_params= self.batch_norm_params):\n",
    "        return None\n",
    "        '''\n",
    "            \n",
    "    def __call__(self, input, reuse=True):\n",
    "        with tf.variable_scope('first_single_conv'):\n",
    "            conv1 = slim.conv2d(input, num_outputs = 64, padding = 'SAME', #[batch_size, 14, 14, 64]\n",
    "                                 kernel_size = [3,3], stride = (2,2),\n",
    "                                 activation_fn = tf.nn.leaky_relu,\n",
    "                                 normalizer_fn=slim.batch_norm, normalizer_params= self.batch_norm_params)\n",
    "\n",
    "            input_projection = slim.conv2d(input, num_outputs=input.get_shape()[3], kernel_size = [3,3], stride=(2,2),\n",
    "                                       activation_fn= None, normalizer_fn= None)\n",
    "            conv1 = tf.concat([conv1, input_projection], axis=3)\n",
    "            print('first_output ', conv1)\n",
    "\n",
    "        with tf.variable_scope('Generator_encoder/' + self.scope, reuse=reuse) as scope:\n",
    "            self.en1 = self.encoder_block(conv1, 1) #[b, 7, 7, 128]\n",
    "            self.en2 = self.encoder_block(en1, 2) #[b, 4, 4, 128]\n",
    "            self.en3 = self.encoder_block(en2, 3) #[b, 2, 2, 128]\n",
    "\n",
    "        return en3\n",
    "                \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
