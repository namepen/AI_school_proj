{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.zeros(shape=[16,28,28,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.zeros(shape=[16,28,28,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(network_variables, name):\n",
    "    \"\"\"\n",
    "    This method counts the tota\n",
    "    l number of unique parameters for a list of variable objects\n",
    "    :param network_variables: A list of tf network variable objects\n",
    "    :param name: Name of the network\n",
    "    \"\"\"\n",
    "    total_parameters = 0\n",
    "    for variable in network_variables:\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            variable_parametes *= dim.value\n",
    "\n",
    "        total_parameters += variable_parametes\n",
    "    print(name, total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(input_features):\n",
    "    \"\"\"\n",
    "    Remove duplicate entries from layer list.\n",
    "    :param input_features: A list of layers\n",
    "    :return: Returns a list of unique feature tensors (i.e. no duplication).\n",
    "    \"\"\"\n",
    "    feature_name_set = set()\n",
    "    non_duplicate_feature_set = []\n",
    "    for feature in input_features:\n",
    "        if feature.name not in feature_name_set:\n",
    "            non_duplicate_feature_set.append(feature)\n",
    "        feature_name_set.add(feature.name)\n",
    "    return non_duplicate_feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, batch_size, layer_sizes, inner_layers, use_wide_connections=False, name=\"d\"):\n",
    "        \"\"\"\n",
    "        Initialize a discriminator network.\n",
    "        :param batch_size: Batch size for discriminator.\n",
    "        :param layer_sizes: A list with the feature maps for each MultiLayer.\n",
    "        :param inner_layers: An integer indicating the number of inner layers.\n",
    "        \"\"\"\n",
    "        self.reuse = False\n",
    "        self.batch_size = batch_size\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.inner_layers = inner_layers\n",
    "        self.conv_layer_num = 0\n",
    "        self.use_wide_connections = use_wide_connections\n",
    "        self.build = True\n",
    "        self.name = name\n",
    "\n",
    "    def upscale(self, x, scale):\n",
    "        \"\"\"\n",
    "            Upscales an image using nearest neighbour\n",
    "            :param x: Input image\n",
    "            :param h_size: Image height size\n",
    "            :param w_size: Image width size\n",
    "            :return: Upscaled image\n",
    "        \"\"\"\n",
    "        [b, h, w, c] = [int(dim) for dim in x.get_shape()]\n",
    "\n",
    "        return tf.image.resize_nearest_neighbor(x, (h * scale, w * scale))\n",
    "\n",
    "    def conv_layer(self, inputs, num_filters, filter_size, strides, activation=None, transpose=False):\n",
    "        \"\"\"\n",
    "        Add a convolutional layer to the network.\n",
    "        :param inputs: Inputs to the conv layer.\n",
    "        :param num_filters: Num of filters for conv layer.\n",
    "        :param filter_size: Size of filter.\n",
    "        :param strides: Stride size.\n",
    "        :param activation: Conv layer activation.\n",
    "        :param transpose: Whether to apply upscale before convolution.\n",
    "        :return: Convolution features\n",
    "        \"\"\"\n",
    "        self.conv_layer_num += 1\n",
    "        if transpose:\n",
    "            outputs = tf.layers.conv2d_transpose(inputs, num_filters, filter_size, strides=strides,\n",
    "                                       padding=\"SAME\", activation=activation)\n",
    "        elif not transpose:\n",
    "            outputs = tf.layers.conv2d(inputs, num_filters, filter_size, strides=strides,\n",
    "                                                 padding=\"SAME\", activation=activation)\n",
    "        return outputs\n",
    "\n",
    "    def add_encoder_layer(self, input, name, training, layer_to_skip_connect, local_inner_layers, num_features,\n",
    "                          dim_reduce=False, dropout_rate=0.0):\n",
    "\n",
    "        \"\"\"\n",
    "        Adds a resnet encoder layer.\n",
    "        :param input: The input to the encoder layer\n",
    "        :param training: Flag for training or validation\n",
    "        :param dropout_rate: A float or a placeholder for the dropout rate\n",
    "        :param layer_to_skip_connect: Layer to skip-connect this layer to\n",
    "        :param local_inner_layers: A list with the inner layers of the current Multi-Layer\n",
    "        :param num_features: Number of feature maps for the convolutions\n",
    "        :param dim_reduce: Boolean value indicating if this is a dimensionality reducing layer or not\n",
    "        :return: The output of the encoder layer\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        [b1, h1, w1, d1] = input.get_shape().as_list()\n",
    "        if layer_to_skip_connect is not None:\n",
    "            [b0, h0, w0, d0] = layer_to_skip_connect.get_shape().as_list()\n",
    "\n",
    "            if h0 > h1:\n",
    "                skip_connect_layer = self.conv_layer(layer_to_skip_connect, int(layer_to_skip_connect.get_shape()[3]),\n",
    "                                                     [3, 3], strides=(2, 2))\n",
    "            else:\n",
    "                skip_connect_layer = layer_to_skip_connect\n",
    "        else:\n",
    "            skip_connect_layer = layer_to_skip_connect\n",
    "        current_layers = [input, skip_connect_layer]\n",
    "        current_layers.extend(local_inner_layers)\n",
    "        current_layers = remove_duplicates(current_layers)\n",
    "        outputs = tf.concat(current_layers, axis=3)\n",
    "        if dim_reduce:\n",
    "            outputs = self.conv_layer(outputs, num_features, [3, 3], strides=(2, 2))\n",
    "            outputs = leaky_relu(features=outputs)\n",
    "            outputs = layer_norm(inputs=outputs, center=True, scale=True)\n",
    "            outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=training)\n",
    "        else:\n",
    "            outputs = self.conv_layer(outputs, num_features, [3, 3], strides=(1, 1))\n",
    "            outputs = leaky_relu(features=outputs)\n",
    "            outputs = layer_norm(inputs=outputs, center=True, scale=True)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def __call__(self, conditional_input, generated_input, training=False, dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        :param conditional_input: A batch of conditional inputs (x_i) of size [batch_size, height, width, channel]\n",
    "        :param generated_input: A batch of generated inputs (x_g) of size [batch_size, height, width, channel]\n",
    "        :param training: Placeholder for training or a boolean indicating training or validation\n",
    "        :param dropout_rate: A float placeholder for dropout rate or a float indicating the dropout rate\n",
    "        :param name: Network name\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        conditional_input = tf.convert_to_tensor(conditional_input)\n",
    "        generated_input = tf.convert_to_tensor(generated_input)\n",
    "        with tf.variable_scope(self.name, reuse=self.reuse):\n",
    "            concat_images = tf.concat([conditional_input, generated_input], axis=3)\n",
    "            outputs = concat_images\n",
    "            encoder_layers = []\n",
    "            current_layers = [outputs]\n",
    "            with tf.variable_scope('conv_layers'):\n",
    "                for i, layer_size in enumerate(self.layer_sizes):\n",
    "                    encoder_inner_layers = [outputs]\n",
    "                    with tf.variable_scope('g_conv{}'.format(i)):\n",
    "                        if i == 0:\n",
    "                            outputs = self.conv_layer(outputs, num_filters=64,\n",
    "                                                      filter_size=(3, 3), strides=(2, 2))\n",
    "                            outputs = leaky_relu(features=outputs)\n",
    "                            outputs = layer_norm(inputs=outputs, center=True, scale=True)\n",
    "                            current_layers.append(outputs)\n",
    "                        else:\n",
    "                            for j in range(self.inner_layers[i]):\n",
    "                                outputs = self.add_encoder_layer(input=outputs,\n",
    "                                                                 name=\"encoder_inner_conv_{}_{}\"\n",
    "                                                                 .format(i, j), training=training,\n",
    "                                                                 layer_to_skip_connect=current_layers[-2],\n",
    "                                                                 num_features=self.layer_sizes[i],\n",
    "                                                                 dropout_rate=dropout_rate,\n",
    "                                                                 dim_reduce=False,\n",
    "                                                                 local_inner_layers=encoder_inner_layers)\n",
    "                                current_layers.append(outputs)\n",
    "                                encoder_inner_layers.append(outputs)\n",
    "                            outputs = self.add_encoder_layer(input=outputs,\n",
    "                                                             name=\"encoder_outer_conv_{}\"\n",
    "                                                             .format(i),\n",
    "                                                             training=training,\n",
    "                                                             layer_to_skip_connect=\n",
    "                                                                     current_layers[-2],\n",
    "                                                             local_inner_layers=\n",
    "                                                                     encoder_inner_layers,\n",
    "                                                             num_features=self.layer_sizes[i],\n",
    "                                                             dropout_rate=dropout_rate,\n",
    "                                                             dim_reduce=True)\n",
    "                            current_layers.append(outputs)\n",
    "                        encoder_layers.append(outputs)\n",
    "\n",
    "\n",
    "            with tf.variable_scope('discriminator_dense_block'):\n",
    "                if self.use_wide_connections:\n",
    "                    mean_encoder_layers = []\n",
    "                    concat_encoder_layers = []\n",
    "                    for layer in encoder_layers:\n",
    "                        mean_encoder_layers.append(tf.reduce_mean(layer, axis=[1, 2]))\n",
    "                        concat_encoder_layers.append(tf.layers.flatten(layer))\n",
    "                    feature_level_flatten = tf.concat(mean_encoder_layers, axis=1)\n",
    "                    location_level_flatten = tf.concat(concat_encoder_layers, axis=1)\n",
    "                else:\n",
    "                    feature_level_flatten = tf.reduce_mean(encoder_layers[-1], axis=[1, 2])\n",
    "                    location_level_flatten = tf.layers.flatten(encoder_layers[-1])\n",
    "\n",
    "                feature_level_dense = tf.layers.dense(feature_level_flatten, units=1024, activation=leaky_relu)\n",
    "                combo_level_flatten = tf.concat([feature_level_dense, location_level_flatten], axis=1)\n",
    "            with tf.variable_scope('discriminator_out_block'):\n",
    "                outputs = tf.layers.dense(combo_level_flatten, 1, name='outputs')\n",
    "\n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n",
    "        #view_names_of_variables(self.variables)\n",
    "        if self.build:\n",
    "            print(\"discr layers\", self.conv_layer_num)\n",
    "            count_parameters(self.variables, name=\"discriminator_parameter_num\")\n",
    "        self.build = False\n",
    "        return outputs, current_layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
