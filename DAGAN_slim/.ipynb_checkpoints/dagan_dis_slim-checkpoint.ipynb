{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.zeros(shape=[16,28,28,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.zeros(shape=[16,28,28,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(network_variables, name):\n",
    "    \"\"\"\n",
    "    This method counts the tota\n",
    "    l number of unique parameters for a list of variable objects\n",
    "    :param network_variables: A list of tf network variable objects\n",
    "    :param name: Name of the network\n",
    "    \"\"\"\n",
    "    total_parameters = 0\n",
    "    for variable in network_variables:\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            variable_parametes *= dim.value\n",
    "\n",
    "        total_parameters += variable_parametes\n",
    "    print(name, total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, batch_size, layer_sizes, inner_layers, use_wide_connections=False, name=\"d\"):\n",
    "        \"\"\"\n",
    "        Initialize a discriminator network.\n",
    "        :param batch_size: Batch size for discriminator.\n",
    "        :param layer_sizes: A list with the feature maps for each MultiLayer.\n",
    "        :param inner_layers: An integer indicating the number of inner layers.\n",
    "        \"\"\"\n",
    "        self.reuse = False\n",
    "        self.batch_size = batch_size\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.inner_layers = inner_layers\n",
    "        self.conv_layer_num = 0\n",
    "        self.use_wide_connections = use_wide_connections\n",
    "        self.build = True\n",
    "        self.name = name\n",
    "        self.current_layers = []\n",
    "        \n",
    "    def upscale(self, x, scale):\n",
    "        \"\"\"\n",
    "            Upscales an image using nearest neighbour\n",
    "            :param x: Input image\n",
    "            :param h_size: Image height size\n",
    "            :param w_size: Image width size\n",
    "            :return: Upscaled image\n",
    "        \"\"\"\n",
    "        [b, h, w, c] = [int(dim) for dim in x.get_shape()]\n",
    "\n",
    "        return tf.image.resize_nearest_neighbor(x, (h * scale, w * scale))\n",
    "        \n",
    "    def encoder_block(self, x1, num): #last output, previous_output \n",
    "        with tf.variable_scope(np.str(num)+'encoder_block'):\n",
    "            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                                num_outputs = 64, padding = 'SAME',\n",
    "                                kernel_size = [3,3], stride = (1,1),\n",
    "                                activation_fn = tf.nn.leaky_relu,\n",
    "                                normalizer_fn=slim.batch_norm, normalizer_params= self.batch_norm_params):\n",
    "\n",
    "                conv1_1 = slim.conv2d(x1)\n",
    "                self.current_layers.append(conv1_1)\n",
    "                output1_1 = tf.concat([conv1_1, x1], axis=3)\n",
    "                print(output1_1)\n",
    "\n",
    "                conv1_2 = slim.conv2d(output1_1)\n",
    "                self.current_layers.append(conv1_2)\n",
    "                output1_2 = tf.concat([conv1_2, output1_1], axis=3)\n",
    "                print(output1_2)\n",
    "\n",
    "                conv1_3 = slim.conv2d(output1_2)\n",
    "                self.current_layers.append(conv1_3)\n",
    "                output1_3 = tf.concat([conv1_3, output1_2], axis=3)\n",
    "                print(output1_3)\n",
    "\n",
    "                conv1_4 = slim.conv2d(output1_3)\n",
    "                self.current_layers.append(conv1_4)\n",
    "                output1_4 = tf.concat([conv1_4, output1_3], axis=3)\n",
    "                print(output1_4)\n",
    "\n",
    "                conv1_5 = slim.conv2d(output1_4, stride=(2,2))\n",
    "                self.current_layers.append(conv1_5)\n",
    "                output = slim.dropout(conv1_5, keep_prob=0.5)\n",
    "                self.current_layers.append(output)\n",
    "                print(output)\n",
    "\n",
    "                if num == 3:\n",
    "                    pass\n",
    "                else :\n",
    "                    input_projection = slim.conv2d(conv1_4, num_outputs=conv1_4.get_shape()[3], stride=(2,2),\n",
    "                                                   activation_fn= None, normalizer_fn= None)\n",
    "                    output = tf.concat([output, input_projection], axis=3)\n",
    "                print(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def __call__(self, conditional_input, generated_input, training=False, dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        :param conditional_input: A batch of conditional inputs (x_i) of size [batch_size, height, width, channel]\n",
    "        :param generated_input: A batch of generated inputs (x_g) of size [batch_size, height, width, channel]\n",
    "        :param training: Placeholder for training or a boolean indicating training or validation\n",
    "        :param dropout_rate: A float placeholder for dropout rate or a float indicating the dropout rate\n",
    "        :param name: Network name\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.batch_norm_params = {'decay' : 0.99, 'scale' : True, 'center' : True,\n",
    "                                 'is_training' : training, 'renorm' : True}\n",
    "        conditional_input = tf.convert_to_tensor(conditional_input)\n",
    "        generated_input = tf.convert_to_tensor(generated_input)\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=self.reuse):\n",
    "            concat_images = tf.concat([conditional_input, generated_input], axis=3)\n",
    "            outputs = concat_images\n",
    "            self.current_layers.append(outputs)\n",
    "               \n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                        num_outputs = 64, padding = 'SAME',\n",
    "                        kernel_size = [3,3], stride = (1,1),\n",
    "                        activation_fn = tf.nn.leaky_relu,\n",
    "                        normalizer_fn=slim.batch_norm, normalizer_params= self.batch_norm_params):\n",
    "            \n",
    "            with tf.variable_scope('first_conv'): \n",
    "                conv = slim.conv2d(outputs, stride=(2,2))\n",
    "                self.current_layers.append(conv)\n",
    "                \n",
    "                input_projection = slim.conv2d(outputs, num_outputs=outputs.get_shape()[3], kernel_size = [3,3], stride=(2,2),\n",
    "                                       activation_fn= None, normalizer_fn= None)\n",
    "                conv1 = tf.concat([conv, input_projection], axis=3)\n",
    "                \n",
    "                en1 = self.encoder_block(conv1, 1)\n",
    "                en2 = self.encoder_block(en1, 2)\n",
    "                en3 = self.encoder_block(en2, 3)\n",
    "                \n",
    "            with tf.variable_scope('decoder_block'):\n",
    "                feature_level_flatten = tf.reduce_mean(en3, axis=[1, 2])\n",
    "                print('else_feature_level_flatten : ', feature_level_flatten)\n",
    "                location_level_flatten = tf.layers.flatten(en3)\n",
    "                print('else_location_level_flatten : ', location_level_flatten)\n",
    "                \n",
    "                feature_level_dense = tf.layers.dense(feature_level_flatten, units=1024, activation=tf.nn.leaky_relu)\n",
    "                combo_level_flatten = tf.concat([feature_level_dense, location_level_flatten], axis=1)\n",
    "                \n",
    "            with tf.variable_scope('discriminator_out_block'):\n",
    "                outputs = tf.layers.dense(combo_level_flatten, 1, name='outputs')\n",
    "                \n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        \n",
    "        if self.build:\n",
    "            print(\"discr layers\", self.conv_layer_num)\n",
    "            count_parameters(self.variables, name=\"discriminator_parameter_num\")\n",
    "        self.build = False\n",
    "        \n",
    "        return outputs, self.current_layers\n",
    "                \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Discriminator(16, [64,64,64,64], [5,5,5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"first_conv/1encoder_block/concat:0\", shape=(16, 14, 14, 134), dtype=float32)\n",
      "Tensor(\"first_conv/1encoder_block/concat_1:0\", shape=(16, 14, 14, 198), dtype=float32)\n",
      "Tensor(\"first_conv/1encoder_block/concat_2:0\", shape=(16, 14, 14, 262), dtype=float32)\n",
      "Tensor(\"first_conv/1encoder_block/concat_3:0\", shape=(16, 14, 14, 326), dtype=float32)\n",
      "Tensor(\"first_conv/1encoder_block/Dropout/dropout_1/mul:0\", shape=(16, 7, 7, 64), dtype=float32)\n",
      "Tensor(\"first_conv/1encoder_block/concat_4:0\", shape=(16, 7, 7, 128), dtype=float32)\n",
      "Tensor(\"first_conv/2encoder_block/concat:0\", shape=(16, 7, 7, 192), dtype=float32)\n",
      "Tensor(\"first_conv/2encoder_block/concat_1:0\", shape=(16, 7, 7, 256), dtype=float32)\n",
      "Tensor(\"first_conv/2encoder_block/concat_2:0\", shape=(16, 7, 7, 320), dtype=float32)\n",
      "Tensor(\"first_conv/2encoder_block/concat_3:0\", shape=(16, 7, 7, 384), dtype=float32)\n",
      "Tensor(\"first_conv/2encoder_block/Dropout/dropout_1/mul:0\", shape=(16, 4, 4, 64), dtype=float32)\n",
      "Tensor(\"first_conv/2encoder_block/concat_4:0\", shape=(16, 4, 4, 128), dtype=float32)\n",
      "Tensor(\"first_conv/3encoder_block/concat:0\", shape=(16, 4, 4, 192), dtype=float32)\n",
      "Tensor(\"first_conv/3encoder_block/concat_1:0\", shape=(16, 4, 4, 256), dtype=float32)\n",
      "Tensor(\"first_conv/3encoder_block/concat_2:0\", shape=(16, 4, 4, 320), dtype=float32)\n",
      "Tensor(\"first_conv/3encoder_block/concat_3:0\", shape=(16, 4, 4, 384), dtype=float32)\n",
      "Tensor(\"first_conv/3encoder_block/Dropout/dropout_1/mul:0\", shape=(16, 2, 2, 64), dtype=float32)\n",
      "Tensor(\"first_conv/3encoder_block/Dropout/dropout_1/mul:0\", shape=(16, 2, 2, 64), dtype=float32)\n",
      "else_feature_level_flatten :  Tensor(\"decoder_block/Mean:0\", shape=(16, 64), dtype=float32)\n",
      "else_location_level_flatten :  Tensor(\"decoder_block/flatten/Reshape:0\", shape=(16, 256), dtype=float32)\n",
      "discr layers 0\n",
      "discriminator_parameter_num 2192331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'discriminator_out_block/outputs/BiasAdd:0' shape=(16, 1) dtype=float32>,\n",
       " [<tf.Tensor 'd/concat:0' shape=(16, 28, 28, 6) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/Conv/LeakyRelu:0' shape=(16, 14, 14, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/1encoder_block/Conv/LeakyRelu:0' shape=(16, 14, 14, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/1encoder_block/Conv_1/LeakyRelu:0' shape=(16, 14, 14, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/1encoder_block/Conv_2/LeakyRelu:0' shape=(16, 14, 14, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/1encoder_block/Conv_3/LeakyRelu:0' shape=(16, 14, 14, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/1encoder_block/Conv_4/LeakyRelu:0' shape=(16, 7, 7, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/1encoder_block/Dropout/dropout_1/mul:0' shape=(16, 7, 7, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/2encoder_block/Conv/LeakyRelu:0' shape=(16, 7, 7, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/2encoder_block/Conv_1/LeakyRelu:0' shape=(16, 7, 7, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/2encoder_block/Conv_2/LeakyRelu:0' shape=(16, 7, 7, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/2encoder_block/Conv_3/LeakyRelu:0' shape=(16, 7, 7, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/2encoder_block/Conv_4/LeakyRelu:0' shape=(16, 4, 4, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/2encoder_block/Dropout/dropout_1/mul:0' shape=(16, 4, 4, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/3encoder_block/Conv/LeakyRelu:0' shape=(16, 4, 4, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/3encoder_block/Conv_1/LeakyRelu:0' shape=(16, 4, 4, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/3encoder_block/Conv_2/LeakyRelu:0' shape=(16, 4, 4, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/3encoder_block/Conv_3/LeakyRelu:0' shape=(16, 4, 4, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/3encoder_block/Conv_4/LeakyRelu:0' shape=(16, 2, 2, 64) dtype=float32>,\n",
       "  <tf.Tensor 'first_conv/3encoder_block/Dropout/dropout_1/mul:0' shape=(16, 2, 2, 64) dtype=float32>])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(x, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
