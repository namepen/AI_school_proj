{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(network_variables, name):\n",
    "    \"\"\"\n",
    "    This method counts the tota\n",
    "    l number of unique parameters for a list of variable objects\n",
    "    :param network_variables: A list of tf network variable objects\n",
    "    :param name: Name of the network\n",
    "    \"\"\"\n",
    "    total_parameters = 0\n",
    "    for variable in network_variables:\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            variable_parametes *= dim.value\n",
    "\n",
    "        total_parameters += variable_parametes\n",
    "    print(name, total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UResNetGenerator:\n",
    "    def __init__(self, layer_sizes, layer_padding, batch_size, num_channels=1,\n",
    "                 inner_layers=0, name=\"g\"):\n",
    "        \"\"\"\n",
    "        Initialize a UResNet generator.\n",
    "        :param layer_sizes: A list with the filter sizes for each MultiLayer e.g. [64, 64, 128, 128]\n",
    "        :param layer_padding: A list with the padding type for each layer e.g. [\"SAME\", \"SAME\", \"SAME\", \"SAME\"]\n",
    "        :param batch_size: An integer indicating the batch size\n",
    "        :param num_channels: An integer indicating the number of input channels\n",
    "        :param inner_layers: An integer indicating the number of inner layers per MultiLayer\n",
    "        \"\"\"\n",
    "        self.reuse = False\n",
    "        self.batch_size = batch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.layer_padding = layer_padding\n",
    "        self.inner_layers = inner_layers\n",
    "        self.conv_layer_num = 0\n",
    "        self.build = True\n",
    "        self.name = name\n",
    "        self.encoder_layers = []\n",
    "        self.decoder_layers = []\n",
    "        self.num_filters = 8\n",
    "        self.batch_norm_params = {'decay' : 0.99, 'scale' : True, 'center' : True,\n",
    "                                 'is_training' : False, 'renorm' : True}\n",
    "        \n",
    "    def upscale(self, x, h_size, w_size):\n",
    "        \"\"\"\n",
    "        Upscales an image using nearest neighbour\n",
    "        :param x: Input image\n",
    "        :param h_size: Image height size\n",
    "        :param w_size: Image width size\n",
    "        :return: Upscaled image\n",
    "        \"\"\"\n",
    "        [b, h, w, c] = [int(dim) for dim in x.get_shape()]\n",
    "\n",
    "        return tf.image.resize_nearest_neighbor(x, (h_size, w_size))\n",
    "    \n",
    "    def encoder_block(self, x1, num, scope = None): \n",
    "        with tf.variable_scope(scope +'/encoder_block'):\n",
    "            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                                num_outputs = 64, padding = 'SAME',\n",
    "                                kernel_size = [3,3], stride = (1,1),\n",
    "                                activation_fn = tf.nn.leaky_relu,\n",
    "                                normalizer_fn=slim.batch_norm, normalizer_params= self.batch_norm_params):\n",
    "                \n",
    "                conv1_1 = slim.conv2d(x1)\n",
    "                output1_1 = tf.concat([conv1_1, x1], axis=3)\n",
    "                \n",
    "                conv1_2 = slim.conv2d(output1_1)\n",
    "                output1_2 = tf.concat([conv1_2, output1_1], axis=3)\n",
    "               \n",
    "                conv1_3 = slim.conv2d(output1_2)\n",
    "                output1_3 = tf.concat([conv1_3, output1_2], axis=3)\n",
    "                \n",
    "                conv1_4 = slim.conv2d(output1_3, stride=(2,2))\n",
    "                output = slim.dropout(conv1_4, keep_prob=0.5)\n",
    "                self.encoder_layers.append(output)\n",
    "                                \n",
    "                if num == 3:\n",
    "                    pass\n",
    "                else :\n",
    "                    input_projection = slim.conv2d(conv1_3, num_outputs=conv1_3.get_shape()[3], stride=(2,2),\n",
    "                                                   activation_fn= None, normalizer_fn= None)\n",
    "                    output = tf.concat([output, input_projection], axis=3)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    def decoder_block(self, x1): \n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                            num_outputs = 64, padding = 'SAME',\n",
    "                            kernel_size = [3,3], stride = (1,1),\n",
    "                            activation_fn = tf.nn.leaky_relu,\n",
    "                            normalizer_fn=slim.batch_norm, normalizer_params= self.batch_norm_params):\n",
    "\n",
    "            conv1_1 = slim.conv2d(x1)\n",
    "            output1_1 = tf.concat([conv1_1, x1], axis=3)\n",
    "            \n",
    "\n",
    "            conv1_2 = slim.conv2d(output1_1)\n",
    "            output1_2 = tf.concat([conv1_2, output1_1], axis=3)\n",
    "            \n",
    "\n",
    "            conv1_3 = slim.conv2d(output1_2)\n",
    "            output1_3 = tf.concat([conv1_3, output1_2], axis=3)\n",
    "            \n",
    "            conv1_4 = slim.conv2d_transpose(output1_3, stride=(2,2))\n",
    "            self.decoder_layers.append(conv1_4)\n",
    "            output = slim.dropout(conv1_4, keep_prob=0.5)\n",
    "\n",
    "            input_projection = slim.conv2d_transpose(conv1_3, num_outputs=conv1_3.get_shape()[3], stride=(2,2),\n",
    "                                           activation_fn= None, normalizer_fn= None)\n",
    "            output = tf.concat([output, input_projection], axis=3)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def z_noise_concat(self, inputs, z_inputs, h, w):\n",
    "        z_dense = tf.layers.dense(z_inputs, h*w*self.num_filters)\n",
    "        z_noise = tf.reshape(z_dense, [self.batch_size, h, w, self.num_filters])\n",
    "        z_noise_concat = tf.concat([inputs, z_noise], axis= 3)\n",
    "        \n",
    "        self.num_filters = np.int(self.num_filters / 2)\n",
    "        \n",
    "        return z_noise_concat\n",
    "    \n",
    "        \n",
    "    def __call__(self, z_inputs, conditional_input, training=False, dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        Apply network on data.\n",
    "        :param z_inputs: Random noise to inject [batch_size, z_dim]\n",
    "        :param conditional_input: A batch of images to use as conditionals [batch_size, height, width, channels]\n",
    "        :param training: Training placeholder or boolean\n",
    "        :param dropout_rate: Dropout rate placeholder or float\n",
    "        :return: Returns x_g (generated images), encoder_layers(encoder features), decoder_layers(decoder features)\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.variable_scope(self.name, reuse=self.reuse):\n",
    "            # reshape from inputs\n",
    "            conditional_input = tf.convert_to_tensor(conditional_input)\n",
    "            \n",
    "            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                       num_outputs = 64, padding = 'SAME',\n",
    "                        kernel_size = [3,3], stride = (1,1), activation_fn = None):\n",
    "                with tf.variable_scope(self.name + 'first_en_conv'):\n",
    "                    conv = slim.conv2d(conditional_input, stride=(2,2))\n",
    "                    self.encoder_layers.append(conv)\n",
    "                    \n",
    "                    input_projection = slim.conv2d(conditional_input, num_outputs=conditional_input.get_shape()[3], stride=(2,2))\n",
    "                    output1 = tf.concat([conv, input_projection], axis=3)\n",
    "\n",
    "                en1 = self.encoder_block(output1, 1, scope='first_en_block') #[B,7, 7, 64]\n",
    "                en2 = self.encoder_block(en1, 2, scope='second_en_block') #[B,4,4,64]\n",
    "                en3 = self.encoder_block(en2, 3, scope='third_en_block') #[b,2,2,64]\n",
    "                #end encoder\n",
    "                \n",
    "                with tf.variable_scope(self.name + '/First_de_conv'):\n",
    "                    self.decoder_layers.append(en3)\n",
    "                    input_noise = self.z_noise_concat(en3, z_inputs, 2, 2)                #[b,2,2,72]\n",
    "                    \n",
    "                with tf.variable_scope(self.name + '/First_de_block'):\n",
    "                    de_conv1 = self.decoder_block(input_noise)                            #[b, 4, 4, 64]\n",
    "                    de_conv1 = tf.concat([de_conv1, self.encoder_layers[2]], axis=3)\n",
    "                    de_conv1_noise = self.z_noise_concat(de_conv1, z_inputs, 4, 4)\n",
    "                    \n",
    "                with tf.variable_scope(self.name + '/Second_de_block'):\n",
    "                    de_conv2 = self.decoder_block(de_conv1_noise)                         #[b, 8, 8, 64]\n",
    "                    de_conv2_noise = self.z_noise_concat(de_conv2, z_inputs, 8, 8)\n",
    "                    de_conv2 = self.upscale(de_conv2_noise, 7, 7)\n",
    "                    de_conv2 = tf.concat([de_conv2, self.encoder_layers[1]], axis=3)\n",
    "                    \n",
    "                with tf.variable_scope(self.name + '/Third_de_block'):\n",
    "                    de_conv3 = self.decoder_block(de_conv2)                               #[b, 14, 14 ,64]\n",
    "                    de_conv3 = tf.concat([de_conv3, self.encoder_layers[0]], axis=3)\n",
    "                    \n",
    "                with tf.variable_scope(self.name + '/Forth_de_block'):\n",
    "                    de_conv4 = self.decoder_block(de_conv3)                               #[b, 28, 28 ,64]\n",
    "                    de_conv4 = tf.concat([de_conv4, conditional_input], axis=3)\n",
    "                    \n",
    "                with tf.variable_scope(self.name + '/Last_de_block'):\n",
    "                    de_conv5_1 = slim.conv2d(de_conv4)\n",
    "                    de_conv5_1 = tf.concat([de_conv5_1, de_conv4], axis=3)\n",
    "                    \n",
    "                    de_conv5_2 = slim.conv2d(de_conv5_1)\n",
    "                    de_conv5_2 = tf.concat([de_conv5_2, de_conv5_1], axis=3)\n",
    "                    \n",
    "                with tf.variable_scope(self.name + '/P_process'):\n",
    "                    de_conv = slim.conv2d(de_conv5_2)\n",
    "                    de_conv = slim.conv2d(de_conv)\n",
    "                \n",
    "                with tf.variable_scope('g_tanh'):\n",
    "                    gan_decoder = tf.tanh(de_conv, name='outputs')\n",
    "                \n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n",
    "\n",
    "        if self.build:\n",
    "            count_parameters(self.variables, 'generator_parameter_num')\n",
    "        self.build = False\n",
    "        \n",
    "        return gan_decoder, self.encoder_layers, self.decoder_layers        \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, batch_size, layer_sizes, inner_layers, use_wide_connections=False, name=\"d\"):\n",
    "        \"\"\"\n",
    "        Initialize a discriminator network.\n",
    "        :param batch_size: Batch size for discriminator.\n",
    "        :param layer_sizes: A list with the feature maps for each MultiLayer.\n",
    "        :param inner_layers: An integer indicating the number of inner layers.\n",
    "        \"\"\"\n",
    "        self.reuse = False\n",
    "        self.batch_size = batch_size\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.inner_layers = inner_layers\n",
    "        self.conv_layer_num = 0\n",
    "        self.use_wide_connections = use_wide_connections\n",
    "        self.build = True\n",
    "        self.name = name\n",
    "        self.is_training = False\n",
    "        self.batch_norm_params = {'decay' : 0.99, 'scale' : True, 'center' : True,\n",
    "                                 'is_training' : self.is_training, 'renorm' : True}\n",
    "        self.current_layers = []\n",
    "        \n",
    "    def upscale(self, x, scale):\n",
    "        \"\"\"\n",
    "            Upscales an image using nearest neighbour\n",
    "            :param x: Input image\n",
    "            :param h_size: Image height size\n",
    "            :param w_size: Image width size\n",
    "            :return: Upscaled image\n",
    "        \"\"\"\n",
    "        [b, h, w, c] = [int(dim) for dim in x.get_shape()]\n",
    "\n",
    "        return tf.image.resize_nearest_neighbor(x, (h * scale, w * scale))\n",
    "        \n",
    "    def encoder_block(self, x1, num): #last output, previous_output \n",
    "        with tf.variable_scope(np.str(num)+'encoder_block'):\n",
    "            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                                num_outputs = 64, padding = 'SAME',\n",
    "                                kernel_size = [3,3], stride = (1,1),\n",
    "                                activation_fn = tf.nn.leaky_relu,\n",
    "                                normalizer_fn=slim.batch_norm, normalizer_params= self.batch_norm_params):\n",
    "\n",
    "                conv1_1 = slim.conv2d(x1)\n",
    "                self.current_layers.append(conv1_1)\n",
    "                output1_1 = tf.concat([conv1_1, x1], axis=3)\n",
    "                print(output1_1)\n",
    "\n",
    "                conv1_2 = slim.conv2d(output1_1)\n",
    "                self.current_layers.append(conv1_2)\n",
    "                output1_2 = tf.concat([conv1_2, output1_1], axis=3)\n",
    "                print(output1_2)\n",
    "\n",
    "                conv1_3 = slim.conv2d(output1_2)\n",
    "                self.current_layers.append(conv1_3)\n",
    "                output1_3 = tf.concat([conv1_3, output1_2], axis=3)\n",
    "                print(output1_3)\n",
    "\n",
    "                conv1_4 = slim.conv2d(output1_3)\n",
    "                self.current_layers.append(conv1_4)\n",
    "                output1_4 = tf.concat([conv1_4, output1_3], axis=3)\n",
    "                print(output1_4)\n",
    "\n",
    "                conv1_5 = slim.conv2d(output1_4, stride=(2,2))\n",
    "                self.current_layers.append(conv1_5)\n",
    "                output = slim.dropout(conv1_5, keep_prob=0.5)\n",
    "                self.current_layers.append(output)\n",
    "                print(output)\n",
    "\n",
    "                if num == 3:\n",
    "                    pass\n",
    "                else :\n",
    "                    input_projection = slim.conv2d(conv1_4, num_outputs=conv1_4.get_shape()[3], stride=(2,2),\n",
    "                                                   activation_fn= None, normalizer_fn= None)\n",
    "                    output = tf.concat([output, input_projection], axis=3)\n",
    "                print(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def __call__(self, conditional_input, generated_input, training=False, dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        :param conditional_input: A batch of conditional inputs (x_i) of size [batch_size, height, width, channel]\n",
    "        :param generated_input: A batch of generated inputs (x_g) of size [batch_size, height, width, channel]\n",
    "        :param training: Placeholder for training or a boolean indicating training or validation\n",
    "        :param dropout_rate: A float placeholder for dropout rate or a float indicating the dropout rate\n",
    "        :param name: Network name\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        conditional_input = tf.convert_to_tensor(conditional_input)\n",
    "        generated_input = tf.convert_to_tensor(generated_input)\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=self.reuse):\n",
    "            concat_images = tf.concat([conditional_input, generated_input], axis=3)\n",
    "            outputs = concat_images\n",
    "            self.current_layers.append(outputs)\n",
    "               \n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                        num_outputs = 64, padding = 'SAME',\n",
    "                        kernel_size = [3,3], stride = (1,1),\n",
    "                        activation_fn = tf.nn.leaky_relu,\n",
    "                        normalizer_fn=slim.batch_norm, normalizer_params= self.batch_norm_params):\n",
    "            \n",
    "            with tf.variable_scope('first_conv'): \n",
    "                conv = slim.conv2d(outputs, stride=(2,2))\n",
    "                self.current_layers.append(conv)\n",
    "                \n",
    "                input_projection = slim.conv2d(outputs, num_outputs=outputs.get_shape()[3], kernel_size = [3,3], stride=(2,2),\n",
    "                                       activation_fn= None, normalizer_fn= None)\n",
    "                conv1 = tf.concat([conv, input_projection], axis=3)\n",
    "                \n",
    "                en1 = self.encoder_block(conv1, 1)\n",
    "                en2 = self.encoder_block(en1, 2)\n",
    "                en3 = self.encoder_block(en2, 3)\n",
    "                \n",
    "            with tf.variable_scope('decoder_block'):\n",
    "                feature_level_flatten = tf.reduce_mean(en3, axis=[1, 2])\n",
    "                print('else_feature_level_flatten : ', feature_level_flatten)\n",
    "                location_level_flatten = tf.layers.flatten(en3)\n",
    "                print('else_location_level_flatten : ', location_level_flatten)\n",
    "                \n",
    "                feature_level_dense = tf.layers.dense(feature_level_flatten, units=1024, activation=tf.nn.leaky_relu)\n",
    "                combo_level_flatten = tf.concat([feature_level_dense, location_level_flatten], axis=1)\n",
    "                \n",
    "            with tf.variable_scope('discriminator_out_block'):\n",
    "                outputs = tf.layers.dense(combo_level_flatten, 1, name='outputs')\n",
    "                \n",
    "        self.reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        \n",
    "        if self.build:\n",
    "            print(\"discr layers\", self.conv_layer_num)\n",
    "            count_parameters(self.variables, name=\"discriminator_parameter_num\")\n",
    "        self.build = False\n",
    "        \n",
    "        return outputs, self.current_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAGAN:\n",
    "    def __init__(self, input_x_i, input_x_j, dropout_rate, generator_layer_sizes,\n",
    "                 discriminator_layer_sizes, generator_layer_padding, z_inputs, batch_size=100, z_dim=100,\n",
    "                 num_channels=1, is_training=True, augment=True, discr_inner_conv=0, gen_inner_conv=0, num_gpus=1, \n",
    "                 use_wide_connections=False):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes a DAGAN object.\n",
    "        :param input_x_i: Input image x_i\n",
    "        :param input_x_j: Input image x_j\n",
    "        :param dropout_rate: A dropout rate placeholder or a scalar to use throughout the network\n",
    "        :param generator_layer_sizes: A list with the number of feature maps per layer (generator) e.g. [64, 64, 64, 64]\n",
    "        :param discriminator_layer_sizes: A list with the number of feature maps per layer (discriminator)\n",
    "                                                                                                   e.g. [64, 64, 64, 64]\n",
    "        :param generator_layer_padding: A list with the type of padding per layer (e.g. [\"SAME\", \"SAME\", \"SAME\",\"SAME\"]\n",
    "        :param z_inputs: A placeholder for the random noise injection vector z (usually gaussian or uniform distribut.)\n",
    "        :param batch_size: An integer indicating the batch size for the experiment.\n",
    "        :param z_dim: An integer indicating the dimensionality of the random noise vector (usually 100-dim).\n",
    "        :param num_channels: Number of image channels\n",
    "        :param is_training: A boolean placeholder for the training/not training flag\n",
    "        :param augment: A boolean placeholder that determines whether to augment the data using rotations\n",
    "        :param discr_inner_conv: Number of inner layers per multi layer in the discriminator\n",
    "        :param gen_inner_conv: Number of inner layers per multi layer in the generator\n",
    "        :param num_gpus: Number of GPUs to use for training\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.z_dim = z_dim\n",
    "        self.z_inputs = z_inputs\n",
    "        self.num_gpus = num_gpus\n",
    "\n",
    "        self.g = UResNetGenerator(batch_size=self.batch_size, layer_sizes=generator_layer_sizes,\n",
    "                                  num_channels=num_channels, layer_padding=generator_layer_padding,\n",
    "                                  inner_layers=gen_inner_conv, name=\"generator\")\n",
    "\n",
    "        self.d = Discriminator(batch_size=self.batch_size, layer_sizes=discriminator_layer_sizes,\n",
    "                               inner_layers=discr_inner_conv, use_wide_connections=use_wide_connections, name=\"discriminator\")\n",
    "\n",
    "        self.input_x_i = input_x_i\n",
    "        self.input_x_j = input_x_j\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.training_phase = is_training\n",
    "        self.augment = augment\n",
    "    def rotate_data(self, image_a, image_b):\n",
    "        \"\"\"\n",
    "        Rotate 2 images by the same number of degrees\n",
    "        :param image_a: An image a to rotate k degrees\n",
    "        :param image_b: An image b to rotate k degrees\n",
    "        :return: Two images rotated by the same amount of degrees\n",
    "        \"\"\"\n",
    "        random_variable = tf.unstack(tf.random_uniform([1], minval=0, maxval=4, dtype=tf.int32, seed=None, name=None))\n",
    "        image_a = tf.image.rot90(image_a, k=random_variable[0])\n",
    "        image_b = tf.image.rot90(image_b, k=random_variable[0])\n",
    "        return [image_a, image_b]\n",
    "\n",
    "    def rotate_batch(self, batch_images_a, batch_images_b): #tf.data.Dataset.shuffle\n",
    "        \"\"\"\n",
    "        Rotate two batches such that every element from set a with the same index as an element from set b are rotated\n",
    "        by an equal amount of degrees\n",
    "        :param batch_images_a: A batch of images to be rotated\n",
    "        :param batch_images_b: A batch of images to be rotated\n",
    "        :return: A batch of images that are rotated by an element-wise equal amount of k degrees\n",
    "        \"\"\"\n",
    "        shapes = map(int, list(batch_images_a.get_shape()))\n",
    "        batch_size, x, y, c = shapes\n",
    "        with tf.name_scope('augment'):\n",
    "            batch_images_unpacked_a = tf.unstack(batch_images_a)\n",
    "            batch_images_unpacked_b = tf.unstack(batch_images_b)\n",
    "            new_images_a = []\n",
    "            new_images_b = []\n",
    "            for image_a, image_b in zip(batch_images_unpacked_a, batch_images_unpacked_b):\n",
    "                rotate_a, rotate_b = self.augment_rotate(image_a, image_b)\n",
    "                new_images_a.append(rotate_a)\n",
    "                new_images_b.append(rotate_b)\n",
    "\n",
    "            new_images_a = tf.stack(new_images_a)\n",
    "            new_images_a = tf.reshape(new_images_a, (batch_size, x, y, c))\n",
    "            new_images_b = tf.stack(new_images_b)\n",
    "            new_images_b = tf.reshape(new_images_b, (batch_size, x, y, c))\n",
    "            return [new_images_a, new_images_b]\n",
    "\n",
    "    def generate(self, conditional_images, z_input=None):\n",
    "        \"\"\"\n",
    "        Generate samples with the DAGAN\n",
    "        :param conditional_images: Images to condition DAGAN on.\n",
    "        :param z_input: Random noise to condition the DAGAN on. If none is used then the method will generate random\n",
    "        noise with dimensionality [batch_size, z_dim]\n",
    "        :return: A batch of generated images, one per conditional image\n",
    "        \"\"\"\n",
    "        if z_input is None:\n",
    "            z_input = tf.random_normal([self.batch_size, self.z_dim], mean=0, stddev=1)\n",
    "\n",
    "        generated_samples, encoder_layers, decoder_layers = self.g(z_input,\n",
    "                               conditional_images,\n",
    "                               training=self.training_phase,\n",
    "                               dropout_rate=self.dropout_rate)\n",
    "        return generated_samples\n",
    "\n",
    "    def augment_rotate(self, image_a, image_b):\n",
    "        r = tf.unstack(tf.random_uniform([1], minval=0, maxval=2, dtype=tf.int32, seed=None, name=None))\n",
    "        rotate_boolean = tf.equal(0, r, name=\"check-rotate-boolean\")\n",
    "        [image_a, image_b] = tf.cond(rotate_boolean[0], lambda: self.rotate_data(image_a, image_b),\n",
    "                        lambda: [image_a, image_b])\n",
    "        return image_a, image_b\n",
    "\n",
    "    def data_augment_batch(self, batch_images_a, batch_images_b):\n",
    "        \"\"\"\n",
    "        Apply data augmentation to a set of image batches if self.augment is set to true\n",
    "        :param batch_images_a: A batch of images to augment\n",
    "        :param batch_images_b: A batch of images to augment\n",
    "        :return: A list of two augmented image batches\n",
    "        \"\"\"\n",
    "        [images_a, images_b] = tf.cond(self.augment, lambda: self.rotate_batch(batch_images_a, batch_images_b),\n",
    "                                       lambda: [batch_images_a, batch_images_b])\n",
    "        return images_a, images_b\n",
    "\n",
    "    def save_features(self, name, features):\n",
    "        \"\"\"\n",
    "        Save feature activations from a network\n",
    "        :param name: A name for the summary of the features\n",
    "        :param features: The features to save\n",
    "        \"\"\"\n",
    "        for i in range(len(features)):\n",
    "            shape_in = features[i].get_shape().as_list()\n",
    "            channels = shape_in[3]\n",
    "            y_channels = 8\n",
    "            x_channels = channels / y_channels\n",
    "\n",
    "            activations_features = tf.reshape(features[i], shape=(shape_in[0], shape_in[1], shape_in[2],\n",
    "                                                                        y_channels, x_channels))\n",
    "\n",
    "            activations_features = tf.unstack(activations_features, axis=4)\n",
    "            activations_features = tf.concat(activations_features, axis=2)\n",
    "            activations_features = tf.unstack(activations_features, axis=3)\n",
    "            activations_features = tf.concat(activations_features, axis=1)\n",
    "            activations_features = tf.expand_dims(activations_features, axis=3)\n",
    "            tf.summary.image('{}_{}'.format(name, i), activations_features)\n",
    "\n",
    "    def loss(self, gpu_id):\n",
    "\n",
    "        \"\"\"\n",
    "        Builds models, calculates losses, saves tensorboard information.\n",
    "        :param gpu_id: The GPU ID to calculate losses for.\n",
    "        :return: Returns the generator and discriminator losses.\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"losses_{}\".format(gpu_id)):\n",
    "\n",
    "            input_a, input_b = self.data_augment_batch(self.input_x_i[gpu_id], self.input_x_j[gpu_id])\n",
    "            x_g = self.generate(input_a)\n",
    "\n",
    "            g_same_class_outputs, g_discr_features = self.d(x_g, input_a, training=self.training_phase,\n",
    "                                          dropout_rate=self.dropout_rate)\n",
    "\n",
    "            t_same_class_outputs, t_discr_features = self.d(input_b, input_a, training=self.training_phase,\n",
    "                                          dropout_rate=self.dropout_rate)\n",
    "\n",
    "            # Remove comments to save discriminator feature activations\n",
    "            # self.save_features(name=\"generated_discr_layers\", features=g_discr_features)\n",
    "            # self.save_features(name=\"real_discr_layers\", features=t_discr_features)\n",
    "\n",
    "            d_real = t_same_class_outputs\n",
    "            d_fake = g_same_class_outputs\n",
    "            d_loss = tf.reduce_mean(d_fake) - tf.reduce_mean(d_real)\n",
    "            g_loss = -tf.reduce_mean(d_fake)\n",
    "\n",
    "            alpha = tf.random_uniform(\n",
    "                shape=[self.batch_size, 1],\n",
    "                minval=0.,\n",
    "                maxval=1.\n",
    "            )\n",
    "            input_shape = input_a.get_shape()\n",
    "            input_shape = [int(n) for n in input_shape]\n",
    "            differences_g = x_g - input_b\n",
    "            differences_g = tf.reshape(differences_g, (self.batch_size, input_shape[1]*input_shape[2]*input_shape[3]))\n",
    "            interpolates_g = input_b + tf.reshape(alpha * differences_g, (self.batch_size, input_shape[1],\n",
    "                                                                          input_shape[2], input_shape[3]))\n",
    "            pre_grads, grad_features = self.d(interpolates_g, input_a, dropout_rate=self.dropout_rate,\n",
    "                                     training=self.training_phase)\n",
    "            gradients = tf.gradients(pre_grads, [interpolates_g, input_a])[0]\n",
    "            slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "            gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n",
    "            d_loss += 10 * gradient_penalty\n",
    "\n",
    "            tf.add_to_collection('g_losses', g_loss)\n",
    "            tf.add_to_collection('d_losses', d_loss)\n",
    "            tf.summary.scalar('g_losses', g_loss)\n",
    "            tf.summary.scalar('d_losses', d_loss)\n",
    "\n",
    "            tf.summary.scalar('d_loss_real', tf.reduce_mean(d_real))\n",
    "            tf.summary.scalar('d_loss_fake', tf.reduce_mean(d_fake))\n",
    "            tf.summary.image('output_generated_images', [tf.concat(tf.unstack(x_g, axis=0), axis=0)])\n",
    "            tf.summary.image('output_input_a', [tf.concat(tf.unstack(input_a, axis=0), axis=0)])\n",
    "            tf.summary.image('output_input_b', [tf.concat(tf.unstack(input_b, axis=0), axis=0)])\n",
    "\n",
    "        return {\n",
    "            \"g_losses\": tf.add_n(tf.get_collection('g_losses'), name='total_g_loss'),\n",
    "            \"d_losses\": tf.add_n(tf.get_collection('d_losses'), name='total_d_loss')\n",
    "        }\n",
    "\n",
    "    def train(self, opts, losses):\n",
    "\n",
    "        \"\"\"\n",
    "        Returns ops for training our DAGAN system.\n",
    "        :param opts: A dict with optimizers.\n",
    "        :param losses: A dict with losses.\n",
    "        :return: A dict with training ops for the dicriminator and the generator.\n",
    "        \"\"\"\n",
    "        opt_ops = dict()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            opt_ops[\"g_opt_op\"] = opts[\"g_opt\"].minimize(losses[\"g_losses\"],\n",
    "                                          var_list=self.g.variables,\n",
    "                                          colocate_gradients_with_ops=True)\n",
    "            opt_ops[\"d_opt_op\"] = opts[\"d_opt\"].minimize(losses[\"d_losses\"],\n",
    "                                                         var_list=self.d.variables,\n",
    "                                                         colocate_gradients_with_ops=True)\n",
    "        return opt_ops\n",
    "\n",
    "    def init_train(self, learning_rate=1e-4, beta1=0.0, beta2=0.9):\n",
    "        \"\"\"\n",
    "        Initialize training by constructing the summary, loss and ops\n",
    "        :param learning_rate: The learning rate for the Adam optimizer\n",
    "        :param beta1: Beta1 for the Adam optimizer\n",
    "        :param beta2: Beta2 for the Adam optimizer\n",
    "        :return: summary op, losses and training ops.\n",
    "        \"\"\"\n",
    "\n",
    "        losses = dict()\n",
    "        opts = dict()\n",
    "\n",
    "        if self.num_gpus > 0:\n",
    "            device_ids = ['/gpu:{}'.format(i) for i in range(self.num_gpus)]\n",
    "        else:\n",
    "            device_ids = ['/cpu:0']\n",
    "        for gpu_id, device_id in enumerate(device_ids):\n",
    "            with tf.device(device_id):\n",
    "                total_losses = self.loss(gpu_id=gpu_id)\n",
    "                for key, value in total_losses.items():\n",
    "                    if key not in losses.keys():\n",
    "                        losses[key] = [value]\n",
    "                    else:\n",
    "                        losses[key].append(value)\n",
    "\n",
    "        for key in list(losses.keys()):\n",
    "            losses[key] = tf.reduce_mean(losses[key], axis=0)\n",
    "            opts[key.replace(\"losses\", \"opt\")] = tf.train.AdamOptimizer(beta1=beta1, beta2=beta2,\n",
    "                                                                            learning_rate=learning_rate)\n",
    "\n",
    "        summary = tf.summary.merge_all()\n",
    "        apply_grads_ops = self.train(opts=opts, losses=losses)\n",
    "\n",
    "        return summary, losses, apply_grads_ops\n",
    "\n",
    "    def sample_same_images(self):\n",
    "        \"\"\"\n",
    "        Samples images from the DAGAN using input_x_i as image conditional input and z_inputs as the gaussian noise.\n",
    "        :return: Inputs and generated images\n",
    "        \"\"\"\n",
    "        conditional_inputs = self.input_x_i[0]\n",
    "        generated = self.generate(conditional_inputs,\n",
    "           z_input=self.z_inputs)\n",
    "\n",
    "        return self.input_x_i[0], generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAGAN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
