{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수정사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sampling.py\n",
    "    line.102 : batch_size -> 1\n",
    "    line.70 : num_generations -> 2\n",
    "                     \n",
    "- experiment_builder.py\n",
    "    line 190 : iter_out in if문\n",
    "    line 250 : 삭제\n",
    "    line 283 : if문 추가 epoch 5당 checkpoint 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import utils.interpolations as interpolations\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from utils.storage import save_statistics, build_experiment_folder\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "from dagan_networks_wgan import *\n",
    "from utils.sampling import sample_generator, sample_two_dimensions_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([self.x_val[val_dict[idx][\"label_idx\"]][val_dict[idx][\"sample_idx\"]]for idx in choose_gen_samples])\n",
    "np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.int(np.sqrt(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([790, 546,  68, 689, 494, 308, 297, 115, 744, 361, 244, 130, 229,\n",
       "        65, 705, 560, 334, 351,  98, 378, 408, 645, 792, 770, 142, 315,\n",
       "        53, 324, 404, 336, 212, 483,  95,  31, 129, 454, 309, 766, 457,\n",
       "       563, 544, 683, 319, 723, 478, 401, 685, 761,  16, 627, 268, 118,\n",
       "       128,  76, 639, 256, 667, 400, 422, 655, 233, 477, 299, 773, 106,\n",
       "       224, 328, 364, 465, 270, 630, 567,  59, 376,  65,  93, 394, 684,\n",
       "       501, 134, 628, 245, 442, 351, 337, 723, 701, 136, 252,  73, 578,\n",
       "       583, 692, 416, 561, 744, 348, 272, 375,  47, 165, 756, 711, 315,\n",
       "       461, 401, 764, 293,  50, 241, 649, 404, 716, 537, 721, 114, 296,\n",
       "       455, 334, 265, 276, 702, 152, 709, 309, 706, 337, 142, 462,  61,\n",
       "       507,  79,  65, 139, 478, 784,  34, 343, 656, 250, 749, 107, 236,\n",
       "       415, 656, 250, 270, 288, 458, 773, 674, 370, 528, 648, 584, 160,\n",
       "       434,  51,  71, 731])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([i for i in range(800)],size=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentBuilder(object):\n",
    "    def __init__(self, args, data):\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.continue_from_epoch = args.continue_from_epoch # -1\n",
    "        self.experiment_name = args.experiment_title # title\n",
    "        self.saved_models_filepath, self.log_path, self.save_image_path = build_experiment_folder(self.experiment_name) #\n",
    "        self.num_gpus = args.num_of_gpus                                          #1 \n",
    "        self.batch_size = args.batch_size                                         #batch_size example 16\n",
    "        gen_depth_per_layer = args.generator_inner_layers                         # 3 \n",
    "        discr_depth_per_layer = args.discriminator_inner_layers                   # 5 \n",
    "        self.z_dim = args.z_dim #100\n",
    "        self.num_generations = args.num_generations                               #64 num_of_generatng samples\n",
    "        self.dropout_rate_value = args.dropout_rate_value\n",
    "        self.data = data\n",
    "        self.reverse_channels = False\n",
    "\n",
    "        generator_layers = [64, 64, 128, 128]\n",
    "        discriminator_layers = [64, 64, 128, 128]\n",
    "\n",
    "        gen_inner_layers = [gen_depth_per_layer, gen_depth_per_layer, gen_depth_per_layer, gen_depth_per_layer]\n",
    "        discr_inner_layers = [discr_depth_per_layer, discr_depth_per_layer, discr_depth_per_layer,\n",
    "                              discr_depth_per_layer]\n",
    "        generator_layer_padding = [\"SAME\", \"SAME\", \"SAME\", \"SAME\"]\n",
    "\n",
    "        image_height = data.image_height\n",
    "        image_width = data.image_width\n",
    "        image_channel = data.image_channel\n",
    "\n",
    "        self.input_x_i = tf.placeholder(tf.float32, [self.num_gpus, self.batch_size, image_height, image_width,\n",
    "                                                     image_channel], 'inputs-1')\n",
    "        self.input_x_j = tf.placeholder(tf.float32, [self.num_gpus, self.batch_size, image_height, image_width,\n",
    "                                                     image_channel], 'inputs-2-same-class')\n",
    "\n",
    "        self.z_input = tf.placeholder(tf.float32, [self.batch_size, self.z_dim], 'z-input')\n",
    "        self.training_phase = tf.placeholder(tf.bool, name='training-flag')\n",
    "        self.random_rotate = tf.placeholder(tf.bool, name='rotation-flag')\n",
    "        self.dropout_rate = tf.placeholder(tf.float32, name='dropout-prob')\n",
    "        self.total_train_batches = int(data.training_data_size / (self.batch_size * self.num_gpus)) 800 / 16 = 50\n",
    "        print('training_data_size', data.training_data_size) #in case, [8,100,28,28,3] -> 8*100 = 800\n",
    "        print('total_train_batches ', self.total_train_batches) \n",
    "        \n",
    "        self.total_gen_batches = int(data.generation_data_size / (self.batch_size * self.num_gpus)) \n",
    "                                    #self.gen_batches * self.batch_size 160 / (16*1) = 10\n",
    "        print('total_gen_batches ', self.total_gen_batches)\n",
    "        \n",
    "        dagan = DAGAN(batch_size=self.batch_size, input_x_i=self.input_x_i, input_x_j=self.input_x_j,\n",
    "                      dropout_rate=self.dropout_rate, generator_layer_sizes=generator_layers,\n",
    "                      generator_layer_padding=generator_layer_padding, num_channels=data.image_channel,\n",
    "                      is_training=self.training_phase, augment=self.random_rotate,\n",
    "                      discriminator_layer_sizes=discriminator_layers,\n",
    "                      discr_inner_conv=discr_inner_layers,\n",
    "                      gen_inner_conv=gen_inner_layers, num_gpus=self.num_gpus, z_dim=self.z_dim, z_inputs=self.z_input,\n",
    "                      use_wide_connections=args.use_wide_connections)\n",
    "\n",
    "        self.summary, self.losses, self.graph_ops = dagan.init_train()\n",
    "        self.same_images = dagan.sample_same_images()\n",
    "\n",
    "        #self.total_train_batches = int(data.training_data_size / (self.batch_size * self.num_gpus))\n",
    "        #print('training_data_size', data.training_data_size)\n",
    "        #print('total_train_batches ', self.total_train_batches)\n",
    "        \n",
    "        #self.total_gen_batches = int(data.generation_data_size / (self.batch_size * self.num_gpus))\n",
    "        #print('total_gen_batches ', self.total_gen_batches)\n",
    "\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.spherical_interpolation = True\n",
    "        self.tensorboard_update_interval = np.int(10)  #In writer.add_summary function \n",
    "        print('tensorboard_update_interval ', self.tensorboard_update_interval) \n",
    "        self.total_epochs = 200\n",
    "\n",
    "        if self.continue_from_epoch == -1:\n",
    "            save_statistics(self.log_path, ['epoch', 'total_d_train_loss_mean', 'total_d_val_loss_mean',\n",
    "                                            'total_d_train_loss_std', 'total_d_val_loss_std',\n",
    "                                            'total_g_train_loss_mean', 'total_g_val_loss_mean',\n",
    "                                            'total_g_train_loss_std', 'total_g_val_loss_std'], create=True)\n",
    "\n",
    "    def run_experiment(self):\n",
    "        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "            sess.run(self.init)\n",
    "            self.train_writer = tf.summary.FileWriter(\"{}/train_logs/\".format(self.log_path),\n",
    "                                                      graph=tf.get_default_graph())\n",
    "            self.validation_writer = tf.summary.FileWriter(\"{}/validation_logs/\".format(self.log_path),\n",
    "                                                           graph=tf.get_default_graph())\n",
    "            self.train_saver = tf.train.Saver()\n",
    "            self.val_saver = tf.train.Saver()\n",
    "\n",
    "            start_from_epoch = 0\n",
    "            if self.continue_from_epoch!=-1: #In case, continue = -1 -> PASS\n",
    "                start_from_epoch = self.continue_from_epoch\n",
    "                checkpoint = \"{}train_saved_model_{}_{}.ckpt\".format(self.saved_models_filepath, self.experiment_name, self.continue_from_epoch)\n",
    "                variables_to_restore = []\n",
    "                for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n",
    "                    print(var)\n",
    "                    variables_to_restore.append(var)\n",
    "\n",
    "                tf.logging.info('Fine-tuning from %s' % checkpoint)\n",
    "\n",
    "                fine_tune = slim.assign_from_checkpoint_fn(\n",
    "                    checkpoint,\n",
    "                    variables_to_restore,\n",
    "                    ignore_missing_vars=True)\n",
    "                fine_tune(sess)\n",
    "\n",
    "            self.iter_done = 0\n",
    "            self.disc_iter = 5\n",
    "            self.gen_iter = 1\n",
    "            best_d_val_loss = np.inf\n",
    "\n",
    "            if self.spherical_interpolation:\n",
    "                dim = int(np.sqrt(self.num_generations)*2)   #11\n",
    "                self.z_2d_vectors = interpolations.create_mine_grid(rows=dim,\n",
    "                                                                    cols=dim,\n",
    "                                                                    dim=self.z_dim, space=3, anchors=None,\n",
    "                                                                    spherical=True, gaussian=True)\n",
    "                self.z_vectors = interpolations.create_mine_grid(rows=1, cols=self.num_generations, dim=self.z_dim,\n",
    "                                                                 space=3, anchors=None, spherical=True, gaussian=True)\n",
    "            else:\n",
    "                self.z_vectors = np.random.normal(size=(self.num_generations, self.z_dim))\n",
    "                self.z_2d_vectors = np.random.normal(size=(self.num_generations, self.z_dim))\n",
    "\n",
    "            with tqdm.tqdm(total=self.total_epochs-start_from_epoch) as pbar_e:  #total = 200\n",
    "                for e in range(start_from_epoch, self.total_epochs):   #0 ~ 200\n",
    "\n",
    "                    train_g_loss = []\n",
    "                    val_g_loss = []\n",
    "                    train_d_loss = []\n",
    "                    val_d_loss = []\n",
    "\n",
    "                    with tqdm.tqdm(total=self.total_train_batches) as pbar_train: #total = 50\n",
    "                        for iter in range(self.total_train_batches): \n",
    "\n",
    "                            cur_sample = 0\n",
    "\n",
    "                            for n in range(self.disc_iter):   # 5\n",
    "                                x_train_i, x_train_j = self.data.get_train_batch()\n",
    "                                x_val_i, x_val_j = self.data.get_val_batch()\n",
    "\n",
    "                                _, d_train_loss_value = sess.run(\n",
    "                                    [self.graph_ops[\"d_opt_op\"], self.losses[\"d_losses\"]],\n",
    "                                    feed_dict={self.input_x_i: x_train_i,\n",
    "                                               self.input_x_j: x_train_j,\n",
    "                                               self.dropout_rate: self.dropout_rate_value,\n",
    "                                               self.training_phase: True, self.random_rotate: True})\n",
    "\n",
    "                                d_val_loss_value = sess.run(\n",
    "                                    self.losses[\"d_losses\"],\n",
    "                                    feed_dict={self.input_x_i: x_val_i,\n",
    "                                               self.input_x_j: x_val_j,\n",
    "                                               self.dropout_rate: self.dropout_rate_value,\n",
    "                                               self.training_phase: False, self.random_rotate: False})\n",
    "\n",
    "                                cur_sample += 1\n",
    "                                train_d_loss.append(d_train_loss_value)\n",
    "                                val_d_loss.append(d_val_loss_value)\n",
    "\n",
    "                            for n in range(self.gen_iter):   # 1\n",
    "                                x_train_i, x_train_j = self.data.get_train_batch()\n",
    "                                x_val_i, x_val_j = self.data.get_val_batch()\n",
    "                                _, g_train_loss_value, train_summaries = sess.run(\n",
    "                                    [self.graph_ops[\"g_opt_op\"], self.losses[\"g_losses\"],\n",
    "                                     self.summary],\n",
    "                                    feed_dict={self.input_x_i: x_train_i,\n",
    "                                               self.input_x_j: x_train_j,\n",
    "                                               self.dropout_rate: self.dropout_rate_value,\n",
    "                                               self.training_phase: True, self.random_rotate: True})\n",
    "\n",
    "                                g_val_loss_value, val_summaries = sess.run(\n",
    "                                    [self.losses[\"g_losses\"], self.summary],\n",
    "                                    feed_dict={self.input_x_i: x_val_i,\n",
    "                                               self.input_x_j: x_val_j,\n",
    "                                               self.dropout_rate: self.dropout_rate_value,\n",
    "                                               self.training_phase: False, self.random_rotate: False})\n",
    "\n",
    "                                cur_sample += 1\n",
    "                                train_g_loss.append(g_train_loss_value)\n",
    "                                val_g_loss.append(g_val_loss_value)\n",
    "\n",
    "                                if iter % (self.tensorboard_update_interval) == 0:\n",
    "                                    self.train_writer.add_summary(train_summaries, global_step=self.iter_done)\n",
    "                                    self.validation_writer.add_summary(val_summaries, global_step=self.iter_done)\n",
    "\n",
    "\n",
    "                            self.iter_done = self.iter_done + 1\n",
    "                            iter_out = \"{}_train_d_loss: {}, train_g_loss: {}, \" \\\n",
    "                                       \"val_d_loss: {}, val_g_loss: {}\".format(self.iter_done,\n",
    "                                                                               d_train_loss_value, g_train_loss_value,\n",
    "                                                                               d_val_loss_value,\n",
    "                                                                               g_val_loss_value)\n",
    "                            pbar_train.set_description(iter_out)\n",
    "                            pbar_train.update(1)\n",
    "\n",
    "                    total_d_train_loss_mean = np.mean(train_d_loss)\n",
    "                    total_d_train_loss_std = np.std(train_d_loss)\n",
    "                    total_g_train_loss_mean = np.mean(train_g_loss)\n",
    "                    total_g_train_loss_std = np.std(train_g_loss)\n",
    "\n",
    "                    print(\n",
    "                        \"Epoch {}: d_train_loss_mean: {}, d_train_loss_std: {},\"\n",
    "                                  \"g_train_loss_mean: {}, g_train_loss_std: {}\"\n",
    "                        .format(e, total_d_train_loss_mean,\n",
    "                                total_d_train_loss_std,\n",
    "                                total_g_train_loss_mean,\n",
    "                                total_g_train_loss_std))\n",
    "\n",
    "                    total_d_val_loss_mean = np.mean(val_d_loss)\n",
    "                    total_d_val_loss_std = np.std(val_d_loss)\n",
    "                    total_g_val_loss_mean = np.mean(val_g_loss)\n",
    "                    total_g_val_loss_std = np.std(val_g_loss)\n",
    "\n",
    "                    print(\n",
    "                        \"Epoch {}: d_val_loss_mean: {}, d_val_loss_std: {},\"\n",
    "                        \"g_val_loss_mean: {}, g_val_loss_std: {}, \"\n",
    "                            .format(e, total_d_val_loss_mean,\n",
    "                                    total_d_val_loss_std,\n",
    "                                    total_g_val_loss_mean,\n",
    "                                    total_g_val_loss_std))\n",
    "\n",
    "\n",
    "\n",
    "                    sample_generator(num_generations=self.num_generations, sess=sess, same_images=self.same_images,\n",
    "                                     inputs=x_train_i,\n",
    "                                     data=self.data, batch_size=self.batch_size, z_input=self.z_input,\n",
    "                                     file_name=\"{}/train_z_variations_{}_{}.png\".format(self.save_image_path,\n",
    "                                                                                        self.experiment_name,\n",
    "                                                                                        e),\n",
    "                                     input_a=self.input_x_i, training_phase=self.training_phase,\n",
    "                                     z_vectors=self.z_vectors, dropout_rate=self.dropout_rate,\n",
    "                                     dropout_rate_value=self.dropout_rate_value)\n",
    "\n",
    "                    sample_two_dimensions_generator(sess=sess,\n",
    "                                                    same_images=self.same_images,\n",
    "                                                    inputs=x_train_i,\n",
    "                                                    data=self.data, batch_size=self.batch_size, z_input=self.z_input,\n",
    "                                                    file_name=\"{}/train_z_spherical_{}_{}\".format(self.save_image_path,\n",
    "                                                                                                  self.experiment_name,\n",
    "                                                                                                  e),\n",
    "                                                    input_a=self.input_x_i, training_phase=self.training_phase,\n",
    "                                                    dropout_rate=self.dropout_rate,\n",
    "                                                    dropout_rate_value=self.dropout_rate_value,\n",
    "                                                    z_vectors=self.z_2d_vectors)\n",
    "                    '''\n",
    "                    with tqdm.tqdm(total=self.total_gen_batches) as pbar_samp: #total = 10\n",
    "                        for i in range(self.total_gen_batches):\n",
    "                            x_gen_a = self.data.get_gen_batch()\n",
    "                            sample_generator(num_generations=self.num_generations, sess=sess,\n",
    "                                             same_images=self.same_images,\n",
    "                                             inputs=x_gen_a,\n",
    "                                             data=self.data, batch_size=self.batch_size, z_input=self.z_input,\n",
    "                                             file_name=\"{}/test_z_variations_{}_{}_{}.png\".format(self.save_image_path,\n",
    "                                                                                                  self.experiment_name,\n",
    "                                                                                                  e, i),\n",
    "                                             input_a=self.input_x_i, training_phase=self.training_phase,\n",
    "                                             z_vectors=self.z_vectors, dropout_rate=self.dropout_rate,\n",
    "                                             dropout_rate_value=self.dropout_rate_value)\n",
    "\n",
    "                            sample_two_dimensions_generator(sess=sess,\n",
    "                                                            same_images=self.same_images,\n",
    "                                                            inputs=x_gen_a,\n",
    "                                                            data=self.data, batch_size=self.batch_size,\n",
    "                                                            z_input=self.z_input,\n",
    "                                                            file_name=\"{}/val_z_spherical_{}_{}_{}\".format(\n",
    "                                                                self.save_image_path,\n",
    "                                                                self.experiment_name,\n",
    "                                                                e, i),\n",
    "                                                            input_a=self.input_x_i,\n",
    "                                                            training_phase=self.training_phase,\n",
    "                                                            dropout_rate=self.dropout_rate,\n",
    "                                                            dropout_rate_value=self.dropout_rate_value,\n",
    "                                                            z_vectors=self.z_2d_vectors)\n",
    "\n",
    "                            pbar_samp.update(1)\n",
    "                        '''\n",
    "\n",
    "                    train_save_path = self.train_saver.save(sess, \"{}/train_saved_model_{}_{}.ckpt\".format(\n",
    "                        self.saved_models_filepath,\n",
    "                        self.experiment_name, e))\n",
    "\n",
    "                    if total_d_val_loss_mean<best_d_val_loss:\n",
    "                        best_d_val_loss = total_d_val_loss_mean\n",
    "                        val_save_path = self.train_saver.save(sess, \"{}/val_saved_model_{}_{}.ckpt\".format(\n",
    "                            self.saved_models_filepath,\n",
    "                            self.experiment_name, e))\n",
    "                        print(\"Saved current best val model at\", val_save_path)\n",
    "\n",
    "                    save_statistics(self.log_path, [e, total_d_train_loss_mean, total_d_val_loss_mean,\n",
    "                                                total_d_train_loss_std, total_d_val_loss_std,\n",
    "                                                total_g_train_loss_mean, total_g_val_loss_mean,\n",
    "                                                total_g_train_loss_std, total_g_val_loss_std])\n",
    "\n",
    "                    pbar_e.update(1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
